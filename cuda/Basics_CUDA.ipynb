{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51789d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=Philosopher:400,700,400italic,700italic' rel='stylesheet' type='text/css'>\r\n",
       "\r\n",
       "<style>\r\n",
       "\r\n",
       "@font-face {\r\n",
       "    font-family: \"Computer Modern\";\r\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "\r\n",
       "/* Formatting for header cells */\r\n",
       ".text_cell_render h1 {\r\n",
       "    font-family: 'Philosopher', sans-serif;\r\n",
       "    font-weight: 400;\r\n",
       "    font-size: 2.2em;\r\n",
       "    line-height: 100%;\r\n",
       "    color: rgb(0, 80, 120);\r\n",
       "    margin-bottom: 0.1em;\r\n",
       "    margin-top: 0.1em;\r\n",
       "    display: block;\r\n",
       "}\t\r\n",
       ".text_cell_render h2 {\r\n",
       "    font-family: 'Philosopher', serif;\r\n",
       "    font-weight: 400;\r\n",
       "    font-size: 1.9em;\r\n",
       "    line-height: 100%;\r\n",
       "    color: rgb(200,100,0);\r\n",
       "    margin-bottom: 0.1em;\r\n",
       "    margin-top: 0.1em;\r\n",
       "    display: block;\r\n",
       "}\t\r\n",
       "\r\n",
       ".text_cell_render h3 {\r\n",
       "    font-family: 'Philosopher', serif;\r\n",
       "    margin-top:12px;\r\n",
       "    margin-bottom: 3px;\r\n",
       "    font-style: italic;\r\n",
       "    color: rgb(94,127,192);\r\n",
       "}\r\n",
       "\r\n",
       ".text_cell_render h4 {\r\n",
       "    font-family: 'Philosopher', serif;\r\n",
       "}\r\n",
       "\r\n",
       ".text_cell_render h5 {\r\n",
       "    font-family: 'Alegreya Sans', sans-serif;\r\n",
       "    font-weight: 300;\r\n",
       "    font-size: 16pt;\r\n",
       "    color: grey;\r\n",
       "    font-style: italic;\r\n",
       "    margin-bottom: .1em;\r\n",
       "    margin-top: 0.1em;\r\n",
       "    display: block;\r\n",
       "}\r\n",
       "\r\n",
       ".text_cell_render h6 {\r\n",
       "    font-family: 'PT Mono', sans-serif;\r\n",
       "    font-weight: 300;\r\n",
       "    font-size: 10pt;\r\n",
       "    color: grey;\r\n",
       "    margin-bottom: 1px;\r\n",
       "    margin-top: 1px;\r\n",
       "}\r\n",
       "\r\n",
       ".CodeMirror{\r\n",
       "        font-family: \"PT Mono\";\r\n",
       "        font-size: 100%;\r\n",
       "}\r\n",
       "\r\n",
       "</style>\r\n",
       "\r\n"
      ],
      "text/plain": [
       "HTML{String}(\"<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\\r\\n<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\\r\\n<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\\r\\n<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\\r\\n<link href='http://fonts.googleapis.com/css?family=Philosopher:400,700,400italic,700italic' rel='stylesheet' type='text/css'>\\r\\n\\r\\n<style>\\r\\n\\r\\n@font-face {\\r\\n    font-family: \\\"Computer Modern\\\";\\r\\n    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\\r\\n}\\r\\n\\r\\n\\r\\n\\r\\n/* Formatting for header cells */\\r\\n.text_cell_render h1 {\\r\\n    font-family: 'Philosopher', sans-serif;\\r\\n    font-weight: 400;\\r\\n    font-size: 2.2em;\\r\\n    line-height: 100%;\\r\\n    color: rgb(0, 80, 120);\\r\\n    margin-bottom: 0.1em;\\r\\n    margin-top: 0.1em;\\r\\n    display: block;\\r\\n}\\t\\r\\n.text_cell_render h2 {\\r\\n    font-family: 'Philosopher', serif;\\r\\n    font-weight: 400;\\r\\n    font-size: 1.9em;\\r\\n    line-height: 100%;\\r\\n    color: rgb(200,100,0);\\r\\n    margin-bottom: 0.1em;\\r\\n    margin-top: 0.1em;\\r\\n    display: block;\\r\\n}\\t\\r\\n\\r\\n.text_cell_render h3 {\\r\\n    font-family: 'Philosopher', serif;\\r\\n    margin-top:12px;\\r\\n    margin-bottom: 3px;\\r\\n    font-style: italic;\\r\\n    color: rgb(94,127,192);\\r\\n}\\r\\n\\r\\n.text_cell_render h4 {\\r\\n    font-family: 'Philosopher', serif;\\r\\n}\\r\\n\\r\\n.text_cell_render h5 {\\r\\n    font-family: 'Alegreya Sans', sans-serif;\\r\\n    font-weight: 300;\\r\\n    font-size: 16pt;\\r\\n    color: grey;\\r\\n    font-style: italic;\\r\\n    margin-bottom: .1em;\\r\\n    margin-top: 0.1em;\\r\\n    display: block;\\r\\n}\\r\\n\\r\\n.text_cell_render h6 {\\r\\n    font-family: 'PT Mono', sans-serif;\\r\\n    font-weight: 300;\\r\\n    font-size: 10pt;\\r\\n    color: grey;\\r\\n    margin-bottom: 1px;\\r\\n    margin-top: 1px;\\r\\n}\\r\\n\\r\\n.CodeMirror{\\r\\n        font-family: \\\"PT Mono\\\";\\r\\n        font-size: 100%;\\r\\n}\\r\\n\\r\\n</style>\\r\\n\\r\\n\")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up a custom stylesheet in IJulia\n",
    "file = open(\"./../style.css\") # A .css file in the same folder as this notebook file\n",
    "styl = read(file, String) # Read the file\n",
    "HTML(\"$styl\") # Output as HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e69fee1",
   "metadata": {},
   "source": [
    "## CUDA.jl (based on [CUDA.jl/ docs](https://cuda.juliagpu.org/stable/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9904a392",
   "metadata": {},
   "source": [
    "### Define a function, struct and  use it inside a kernel \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c7bc4",
   "metadata": {},
   "source": [
    "#### CPU implementation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b217750c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2098186"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CUDA\n",
    "\n",
    "struct Index\n",
    "    ix::Integer\n",
    "    iy::Integer\n",
    "    iz::Integer\n",
    "    \n",
    "    function Index(indexes)\n",
    "        new(indexes[1], indexes[2], indexes[3])\n",
    "    end\n",
    "end\n",
    "\n",
    "# Function to map\n",
    "function map_index(index, dim_img)\n",
    "    onedim_index = (dim_img[1]*dim_img[2])*(index.iz - 1) + (dim_img[1])*(index.iy - 1) + index.ix\n",
    "    return onedim_index\n",
    "end\n",
    "\n",
    "# CPU execution\n",
    "dim_img = [1024, 1024, 100]\n",
    "index = Index([10,2,3])\n",
    "\n",
    "map_index(index, dim_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40512f8",
   "metadata": {},
   "source": [
    "#### GPU implementation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3932cf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Performing scalar indexing on task Task (runnable) @0x00007fbc59178e70.\n",
      "│ Invocation of getindex resulted in scalar indexing of a GPU array.\n",
      "│ This is typically caused by calling an iterating implementation of a method.\n",
      "│ Such implementations *do not* execute on the GPU, but very slowly on the CPU,\n",
      "│ and therefore are only permitted from the REPL for prototyping purposes.\n",
      "│ If you did intend to index this array, annotate the caller with @allowscalar.\n",
      "└ @ GPUArraysCore /home/mvanzulli/.julia/packages/GPUArraysCore/rSIl2/src/GPUArraysCore.jl:81\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(10, 2, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform variables to cuda arrays\n",
    "dim_img_gpu = CuArray(dim_img)\n",
    "index_gpu = Index(CuArray([10,2,3]))\n",
    "# try to access using \n",
    "# we obtain: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fabb82f",
   "metadata": {},
   "source": [
    "This warning is a common pitfall for new incoming users, we really don't want to execute indexing in CPU, for instance for loops in cpu with CuArrays, because this will copy CuArray back to CPU, iterate and the copying it back to the GPU, what is really slow.  Also we can switch off indexing using `CUDA.allowscalar(fals)` to avoid indexing with CPU arrays. New versions doesn't allow to use indexing besides interactive execution modes. \n",
    "\n",
    "How to fix this? The answer is, that there is a conversion mechanism, which adapts objects into CUDA compatible bitstypes. It is based on the `Adapt.jl`\n",
    "package and basic types like CuArray already participate in this mechanism. For custom types, we just need to add a conversion rule like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d1e5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Adapt \n",
    "\n",
    "function Adapt.adapt_structure(to, index::Index)\n",
    "    ix = Adapt.adapt_strucutre(to, index.ix)\n",
    "    iy = Adapt.adapt_strucutre(to, index.iy)\n",
    "    iz = Adapt.adapt_strucutre(to, index.iz)\n",
    "    return Index(ix, iy, iz)\n",
    "end\n",
    "\n",
    "#or \n",
    "# Alternatively instead of defining Adapt.adapt_structure explictly,\n",
    "# we could have done\n",
    "Adapt.@adapt_structure Index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc27b95",
   "metadata": {},
   "source": [
    "### CuArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c372a5e",
   "metadata": {},
   "source": [
    "The CuArray type is an essential part of the toolchain. Is a resemble of Array. Primarily, it is used to manage GPU memory, and copy data from and back to the CPU. When we use `CuArray(variable)` then two operations are simultaneously being executed behind the scenes, memory allocation and copy into device.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f075ea5",
   "metadata": {},
   "source": [
    "#### ax+b = y example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcf637ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: z_cpu == a * ones(Float32, dim) + ones(Float32, dim)\n",
       "   Evaluated: Float32[4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593  …  4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593] == Float32[4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593  …  4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the SAXPY product ax +b = c\n",
    "# define constant variables \n",
    "const dim = 100_000_000\n",
    "const a = pi \n",
    "# define a cu array\n",
    "x = CUDA.ones(Float32, dim)\n",
    "z = CuArray{Float32}(undef,dim)\n",
    "# essential transformations with CuArrays\n",
    "y = copy(x)\n",
    "fill!(x,1)\n",
    "# compute ax+b=z and we force the CPU to wait GPU finalization\n",
    "CUDA.@sync z .= a.*x .+ y\n",
    "# copy the result to CPU\n",
    "z_cpu = Array(z)\n",
    "# check results\n",
    "import Test\n",
    "Test.@test z_cpu == (a*ones(Float32, dim) + ones(Float32, dim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e7239",
   "metadata": {},
   "source": [
    "### Operators bult-in kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1dc7f7",
   "metadata": {},
   "source": [
    "There are some operators that we can use without writing an specific kernel for it, among them we dispose:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4796144f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: abs(result_cpu[1] .- result_gpu[1]) < 1.0e-5\n",
       "   Evaluated: 3.159911643457747e-8 < 1.0e-5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cuArrays\n",
    "α = CUDA.zeros(1024)\n",
    "β = CUDA.ones(1024)\n",
    "# CPUArrays\n",
    "a_cpu = zeros(1024)\n",
    "b_cpu = ones(1024)\n",
    "\n",
    "result_cpu = a_cpu.^2 .+ sin.(b_cpu) \n",
    "result_gpu = α.^2 .+ sin.(β)\n",
    "\n",
    "Test.@test abs(result_cpu[1] .- result_gpu[1]) < 1e-5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f60d1",
   "metadata": {},
   "source": [
    "When possible, these operations integrate with existing vendor libraries such as CUBLAS and CURAND. For example, multiplying matrices or generating random numbers will automatically dispatch to these high-quality libraries, if types are supported, and fall back to generic implementations otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606fa27f",
   "metadata": {},
   "source": [
    "### Vendor libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa23310a",
   "metadata": {},
   "source": [
    "For actually useful operations we can perform computations integrated with some NIVIDA's libraries that provide precompiled kernels for many operations. Among them we can use: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a690ff58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500×1 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 230.63507\n",
       " 239.95924\n",
       " 239.69102\n",
       " 241.92593\n",
       " 241.84622\n",
       " 241.20676\n",
       " 246.0498\n",
       " 232.88232\n",
       " 248.21104\n",
       " 245.14676\n",
       " 252.61493\n",
       " 249.52919\n",
       " 240.1671\n",
       "   ⋮\n",
       " 240.44585\n",
       " 247.27472\n",
       " 247.73047\n",
       " 249.997\n",
       " 238.55968\n",
       " 237.96555\n",
       " 239.73299\n",
       " 241.67624\n",
       " 240.30276\n",
       " 241.93796\n",
       " 251.66449\n",
       " 242.6794"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CUBLAS \n",
    "using LinearAlgebra\n",
    "# perofrm A*B = Y\n",
    "M = 500\n",
    "N = 1_000\n",
    "Y = CuArray{Float32}(undef, (M,1))\n",
    "A = CUDA.rand(M,N)\n",
    "B = CUDA.rand(N,1)\n",
    "# use mul! implemented function \n",
    "mul!(Y, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d62459c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.CUSOLVER.CuQR{Float32, CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}} with factors Q and R:\n",
       "Float32[-0.0084267855 0.009119287 … -0.014994912 -0.051567167; -0.028107788 -0.03481579 … -0.057056278 0.026883157; … ; -0.025895227 -0.046343774 … 0.029530665 0.017978534; -0.04862549 0.03881343 … 0.014941741 -0.03823671]\n",
       "Float32[-13.179489 -9.867693 … -9.661791 -10.453778; 0.0 -8.631413 … -3.5047283 -2.944806; … ; 0.0 0.0 … 0.1839259 -0.021541096; 0.0 0.0 … -0.07726628 -0.47282785]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CUSOLVER \n",
    "qr(A)\n",
    "# We have m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c899ac",
   "metadata": {},
   "source": [
    "Writing wrappers for these libraries and integrating them with the relevant Julia interfaces or packages is not a difficult, but a very time consuming job.\n",
    "\n",
    "To help with that, `CUDA.jl` also exposes all of the underlying C APIs, and makes them compatible with the `CuArray` type. For example, let's find the index of the smallest value using the cublasIsamin function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1687800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "433586"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets find the index of he salles value using culbasIsamin function f\n",
    "out = Ref{Cint}()\n",
    "CUBLAS.cublasIsamin_v2(CUBLAS.handle(), length(A), A, stride(A, 1), out)\n",
    "out[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03c2266",
   "metadata": {},
   "source": [
    "### Kernel programming "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8877fbd",
   "metadata": {},
   "source": [
    "But sometimes we need to declare and write our own kernels. Kernels are functions that are executed in a massively parallel fashion, and are launched by using the `@cuda` macro. The kernels are programmed with the SPMD simple program (the kernel) executed through Multiple Data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ded2835d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: Array(foo) == ones(1024)\n",
       "   Evaluated: Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] == [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = CUDA.zeros(1024)\n",
    "\n",
    "function kernel!(a)\n",
    "    i = threadIdx().x\n",
    "    a[i] += 1\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "@cuda threads = length(foo) kernel!(foo)\n",
    "# copy to cpu\n",
    "Test.@test Array(foo) == ones(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6895320",
   "metadata": {},
   "source": [
    "### y = ax+b example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "649ce459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: z_cpu == a * ones(Float32, dim) + ones(Float32, dim)\n",
       "   Evaluated: Float32[4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593  …  4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593] == Float32[4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593  …  4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function axby(z, a, x, y)\n",
    "    i = (blockIdx().x -1)* blockDim().x + threadIdx().x\n",
    "    i <= length(z) && @inbounds z[i] = a * x[i] + y[i]\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "# define the GPU execution parameters\n",
    "nthreads = CUDA.attribute(device(),\n",
    "    CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK)\n",
    "# compute the number of blocks that overlay the dimenssion of arrays\n",
    "nblocks = cld(dim, nthreads)\n",
    "\n",
    "CUDA.@sync @cuda(\n",
    "    threads = nthreads,\n",
    "    blocks = nblocks,\n",
    "    axby(z,a,x,y)\n",
    ")\n",
    "# copy the result to CPU\n",
    "z_cpu = Array(z)\n",
    "# check results\n",
    "import Test\n",
    "Test.@test z_cpu == (a*ones(Float32, dim) + ones(Float32, dim))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1f5ac7",
   "metadata": {},
   "source": [
    "Some Julia features are not included when we are programming a kernel, such as: \n",
    "\n",
    "- allocate memory\n",
    "- I/O is disallowed \n",
    "- badly-typed code will not compile \n",
    "\n",
    "Also we have to be aware that: \n",
    "- we need to **respect hardware limitations** \n",
    "- we need to efficiently use hardware resources **(occupancy)**\n",
    "- **not every operation maps cleanly** on a scalar kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ad64d6",
   "metadata": {},
   "source": [
    "### Hardware limitations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49938737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "CUDA error: invalid argument (code 1, ERROR_INVALID_VALUE)",
     "output_type": "error",
     "traceback": [
      "CUDA error: invalid argument (code 1, ERROR_INVALID_VALUE)",
      "",
      "Stacktrace:",
      "  [1] throw_api_error(res::CUDA.cudaError_enum)",
      "    @ CUDA ~/.julia/packages/CUDA/tTK8Y/lib/cudadrv/error.jl:89",
      "  [2] macro expansion",
      "    @ ~/.julia/packages/CUDA/tTK8Y/lib/cudadrv/error.jl:97 [inlined]",
      "  [3] cuLaunchKernel(f::CuFunction, gridDimX::UInt32, gridDimY::UInt32, gridDimZ::UInt32, blockDimX::UInt32, blockDimY::UInt32, blockDimZ::UInt32, sharedMemBytes::Int64, hStream::CuStream, kernelParams::Vector{Ptr{Nothing}}, extra::Ptr{Nothing})",
      "    @ CUDA ~/.julia/packages/CUDA/tTK8Y/lib/utils/call.jl:26",
      "  [4] #39",
      "    @ ~/.julia/packages/CUDA/tTK8Y/lib/cudadrv/execution.jl:69 [inlined]",
      "  [5] macro expansion",
      "    @ ~/.julia/packages/CUDA/tTK8Y/lib/cudadrv/execution.jl:33 [inlined]",
      "  [6] macro expansion",
      "    @ ./none:0 [inlined]",
      "  [7] pack_arguments(::CUDA.var\"#39#40\"{Bool, Int64, CuStream, CuFunction, CuDim3, CuDim3}, ::CUDA.KernelState, ::CuDeviceVector{Int64, 1}, ::CuDeviceVector{Int64, 1}, ::CuDeviceVector{Int64, 1})",
      "    @ CUDA ./none:0",
      "  [8] #launch#38",
      "    @ ~/.julia/packages/CUDA/tTK8Y/lib/cudadrv/execution.jl:62 [inlined]",
      "  [9] #44",
      "    @ ~/.julia/packages/CUDA/tTK8Y/lib/cudadrv/execution.jl:136 [inlined]",
      " [10] macro expansion",
      "    @ ~/.julia/packages/CUDA/tTK8Y/lib/cudadrv/execution.jl:95 [inlined]",
      " [11] macro expansion",
      "    @ ./none:0 [inlined]",
      " [12] convert_arguments",
      "    @ ./none:0 [inlined]",
      " [13] #cudacall#43",
      "    @ ~/.julia/packages/CUDA/tTK8Y/lib/cudadrv/execution.jl:135 [inlined]",
      " [14] macro expansion",
      "    @ ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:204 [inlined]",
      " [15] macro expansion",
      "    @ ./none:0 [inlined]",
      " [16] call(::CUDA.HostKernel{typeof(axby), Tuple{CuDeviceVector{Int64, 1}, Irrational{:π}, CuDeviceVector{Int64, 1}, CuDeviceVector{Int64, 1}}}, ::CuDeviceVector{Int64, 1}, ::Irrational{:π}, ::CuDeviceVector{Int64, 1}, ::CuDeviceVector{Int64, 1}; call_kwargs::Base.Pairs{Symbol, Int64, Tuple{Symbol, Symbol}, NamedTuple{(:threads, :blocks), Tuple{Int64, Int64}}})",
      "    @ CUDA ./none:0",
      " [17] (::CUDA.HostKernel{typeof(axby), Tuple{CuDeviceVector{Int64, 1}, Irrational{:π}, CuDeviceVector{Int64, 1}, CuDeviceVector{Int64, 1}}})(::CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}, ::Vararg{Any}; threads::Int64, blocks::Int64, kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ CUDA ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:484",
      " [18] macro expansion",
      "    @ ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:104 [inlined]",
      " [19] top-level scope",
      "    @ ~/.julia/packages/CUDA/tTK8Y/src/utilities.jl:25",
      " [20] eval",
      "    @ ./boot.jl:373 [inlined]",
      " [21] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "# lets check what happens if we have really big arrays \n",
    "x = CuArray(1:dim)\n",
    "y = CuArray(1:dim)\n",
    "z = similar(x)\n",
    "\n",
    "nthreads = 2000\n",
    "nblocks = cld(dim, nthreads)\n",
    "\n",
    "CUDA.@sync @cuda(\n",
    "    threads = nthreads,\n",
    "    blocks = nblocks,\n",
    "    axby(z,a,x,y)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb5b50c",
   "metadata": {},
   "source": [
    "When we obtain this `ERROR_INVALID_VALUE` is because we have more threads than is alllwoed, we can insepct the maximum using \n",
    "\n",
    "```\n",
    "nthreads = CUDA.attribute(device(),\n",
    "    CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK)\n",
    "```\n",
    "\n",
    "However using more complicated kernels local memory, shared memory may not allow to allocate all required memory. In order to solve that shortage we have an `Occupancy API`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eb1785",
   "metadata": {},
   "source": [
    "### Occupancy API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e73a760",
   "metadata": {},
   "source": [
    "We can figured it out the thread limit by compiling the kernel before lunching it to inspect its properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d35412d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tha max number of threads that this kernel allows is 1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(blocks = 60, threads = 768)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_kernel = @cuda( \n",
    "    launch = false,\n",
    "    axby(z,a,x,y)\n",
    ")\n",
    "\n",
    "println(\"Tha max number of threads that this kernel allows is $(CUDA.maxthreads(compiled_kernel))\")\n",
    "kernel_config = CUDA.launch_configuration(compiled_kernel.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c760c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we now can set the threads and blocks using the occupancy API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91d17da2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threads = min(length(x), kernel_config.threads) = 768\n",
      "blocks = cld(length(x), kernel_config.threads) = 1303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1303"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@show threads = min(length(x), kernel_config.threads )\n",
    "@show blocks = cld(length(x), kernel_config.threads )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765cfe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we can reuse the pre_complied kerne using "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84db653",
   "metadata": {},
   "source": [
    "### CUDA. BLAS solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe52f22",
   "metadata": {},
   "source": [
    "We can call CUBLAS module inside CUDA which provides all kinds of linear algebra functionalities for our CUDA apps: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "858c73ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: y == a * ones(Float32, dim) + ones(Float32, dim)\n",
       "   Evaluated: Float32[4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593  …  4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593] == Float32[4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593  …  4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593, 4.141593]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CUDA, CUDA.CUBLAS \n",
    "\n",
    "x = CUDA.ones(Float32, dim)\n",
    "y = CUDA.ones(Float32, dim)\n",
    "\n",
    "# perform saxpy and overwrite y vector with axpy CUBLAS function\n",
    "CUDA.@sync CUBLAS.axpy!(dim, a, x, y)\n",
    "\n",
    "# copy results to cpu\n",
    "y = Array(y)\n",
    "\n",
    "Test.@test y == (a*ones(Float32, dim) + ones(Float32, dim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90446c88",
   "metadata": {},
   "source": [
    "### CUDA API wrappers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f7ad7e",
   "metadata": {},
   "source": [
    "For advanced use of the CUDA, you can use the driver API wrappers in CUDA.jl. Common operations include synchronizing the GPU, inspecting its properties, starting the profiler, etc. These operations are low-level, but for your convenience wrapped using high-level constructs. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa46412e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capability(device) = v\"8.6.0\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Calling CUDA.@profile only informs an external profiler to start.\n",
      "│ The user is responsible for launching Julia under a CUDA profiler.\n",
      "│ \n",
      "│ It is recommended to use Nsight Systems, which supports interactive profiling:\n",
      "│ $ nsys launch julia\n",
      "└ @ CUDA.Profile /home/mvanzulli/.julia/packages/CUDA/tTK8Y/lib/cudadrv/profile.jl:82\n"
     ]
    }
   ],
   "source": [
    "CUDA.@profile begin\n",
    "    # code that runs under the profiler\n",
    "end\n",
    "\n",
    "# or execute this to show wich capabilitie\n",
    "# s are avialable accordint to the device\n",
    "\n",
    "for device in CUDA.devices()\n",
    "    @show capability(device)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
