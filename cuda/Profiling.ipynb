{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56085416",
   "metadata": {},
   "source": [
    "# Profiling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01510308",
   "metadata": {},
   "source": [
    "Often is really important to obtain a profiling for our GPU program, to check coalsceed access, race conditions problems, memory managment, etc. For such task, we can call `nvprof` tool from NVIDIA. On a Unix system we should execute:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9edae8",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ nvprof --profile-from-start off /path/to/julia\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dba162",
   "metadata": {},
   "source": [
    "The `/path/to/julia` is the path to julia binary. Note that we don't initialize immediately the profiler but we can call the CUDA API's with the macro @profile:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fdb19a",
   "metadata": {},
   "source": [
    "```julia\n",
    "CUDA.@profile kernel_name(x_d, y_d, r_d)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192595e2",
   "metadata": {},
   "source": [
    "But nvprof is not longer used for GPUs with compute capalities newer than 7.0, instead we need nsys (Nsight system), to set nsys to julia we run: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56123830",
   "metadata": {},
   "source": [
    "```bash\n",
    "nsys launch julia --trace=cuda \n",
    "```\n",
    "then we can execute the previous code but adding the macro CUDA.@profile before we lunch the kernel:\n",
    "```julia \n",
    "julia> using CUDA\n",
    "\n",
    "julia> a = CUDA.rand(1024,1024,1024);\n",
    "\n",
    "julia> sin.(a);\n",
    "\n",
    "julia> CUDA.@profile sin.(a);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046c8a62",
   "metadata": {},
   "source": [
    "Then we open Nshight System using `nsys-cli` and open the report. If we want the old nvprof output we can execute "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cef7e0",
   "metadata": {},
   "source": [
    "```bash\n",
    "nsys nvprof --profile-from-start off /path/to/julia --trace=cuda\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187dadc1",
   "metadata": {},
   "source": [
    "then we can execute the previous code or the kernel we want to profile adding the macro CUDA.@profile before we lunch the kernel:\n",
    "```julia \n",
    "julia> using CUDA\n",
    "\n",
    "julia> a = CUDA.rand(1024,1024,1024);\n",
    "\n",
    "julia> sin.(a);\n",
    "\n",
    "julia> CUDA.@profile sin.(a);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710a982e",
   "metadata": {},
   "source": [
    "## Profiling a reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5399d4",
   "metadata": {},
   "source": [
    "We paste here our both atomic and shared mem versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28558455",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reduce_grid_shared (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools, CUDA, Test\n",
    "\n",
    "#######################\n",
    "# REDUCE GRID ATOMIC\n",
    "#######################\n",
    "function reduce_grid_atomic(op, a, b)\n",
    "    num_elements = blockDim().x*2\n",
    "    thread = threadIdx().x\n",
    "    block = blockIdx().x\n",
    "    \n",
    "    #parallel reduction of values in a block (stride or distance between each thread reduction) \n",
    "    stride_threads = 1\n",
    "    # parallel reduction between blocks has a stride of \n",
    "    stride_blocks = (block - 1)*num_elements\n",
    "\n",
    "    \n",
    "    # while still have elements to reduce \n",
    "    while stride_threads < num_elements\n",
    "        # add a barrier to sync threads\n",
    "        sync_threads()\n",
    "        # compute index to reduce \n",
    "        index = 2*stride_threads*(thread - 1) + 1 \n",
    "        # check index and index + d are inbounds a\n",
    "        @inbounds if index ≤ num_elements && index + stride_threads + stride_blocks ≤ length(a)\n",
    "#             CUDA.@cuprintln (\"thread $thread: a[$index] + a[$(index + stride_blocks)] = $(a[index] + a[index + stride_blocks])\")\n",
    "            a[stride_blocks + index] = op(a[index + stride_blocks], a[index + stride_threads + stride_blocks])\n",
    "        end\n",
    "        stride_threads *= 2\n",
    "    end\n",
    "    # do attomic operatios with the first entry of ech block (sum through each block)\n",
    "    if thread == 1 \n",
    "        CUDA.@atomic b[] = op(b[], a[stride_blocks + 1])\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "# define test inputs\n",
    "c_a = 1:16\n",
    "d_a = CuArray(1:16)\n",
    "d_b = CuArray([0])\n",
    "# lunch kernel\n",
    "@cuda(\n",
    "    threads = 2,\n",
    "    blocks = 4,\n",
    "    reduce_grid_atomic(+, d_a, d_b)\n",
    "    )\n",
    "# test the result \n",
    "using Test\n",
    "CUDA.@allowscalar d_b\n",
    "@test CUDA.@allowscalar d_b[] == sum(c_a)\n",
    "\n",
    "\n",
    "#######################\n",
    "# REDUCE SHARED MEMORY\n",
    "#######################\n",
    "function reduce_grid_shared(op, a::AbstractArray{T}, b) where {T}\n",
    "    num_elements = blockDim().x*2\n",
    "    thread = threadIdx().x\n",
    "    block = blockIdx().x\n",
    "    #parallel reduction of values in a block (stride or distance between each thread reduction) \n",
    "    stride_threads = 1\n",
    "    # parallel reduction between blocks has a stride of \n",
    "    stride_blocks = (block - 1)*num_elements\n",
    "    \n",
    "    # shared mem to buffer the a elements\n",
    "    shared = @cuStaticSharedMem(T, (2048,))\n",
    "    @inbounds shared[thread] = a[thread + stride_blocks]\n",
    "    @inbounds shared[thread + blockDim().x] = a[thread + stride_blocks + blockDim().x]\n",
    " \n",
    "    # while still have elements to reduce \n",
    "    while stride_threads < num_elements\n",
    "        # add a barrier to sync threads\n",
    "        sync_threads()\n",
    "        # compute index to reduce \n",
    "        index = 2*stride_threads*(thread - 1) + 1 \n",
    "        # check index and index + d are inbounds a\n",
    "        @inbounds if index ≤ num_elements && index + stride_threads + stride_blocks ≤ length(a)\n",
    "            shared[index] = op(shared[index], shared[index + stride_threads])\n",
    "        end\n",
    "        stride_threads *= 2\n",
    "    end\n",
    "    # do attomic operatios with the first entry of ech block reduction at shared \n",
    "    if thread == 1 \n",
    "        CUDA.@atomic b[] = op(b[], shared[1])\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fe12d3",
   "metadata": {},
   "source": [
    "### Testing both reduce implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552e06c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: \u001b[90m#= In[3]:25 =#\u001b[39m CUDA.@allowscalar d_b[] == sum(c_a)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define test inputs\n",
    "c_a = 1:16\n",
    "d_a = CuArray(1:16)\n",
    "d_b = CuArray([0])\n",
    "# lunch kernel shared\n",
    "@cuda(\n",
    "    threads = 4,\n",
    "    blocks = 2,\n",
    "    reduce_grid_atomic(+, d_a, d_b)\n",
    "    )\n",
    "# test the result \n",
    "@test CUDA.@allowscalar d_b[] == sum(c_a)\n",
    "# re-define test inputs\n",
    "CUDA.unsafe_free!(d_b)\n",
    "CUDA.unsafe_free!(d_a)\n",
    "d_a = CuArray(1:16)\n",
    "d_b = CuArray([0])\n",
    "\n",
    "# lunch kernel shared\n",
    "@cuda(\n",
    "    threads = 4,\n",
    "    blocks = 2,\n",
    "    reduce_grid_shared(+, d_a, d_b)\n",
    "    )\n",
    "@test CUDA.@allowscalar d_b[] == sum(c_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537aae63",
   "metadata": {},
   "source": [
    "Then we just define two different functions first `main()` function to profile our kernels and then `my_reduce()` to call both reducntion kernels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fccaa62f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_reduce (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function my_reduce(op::Function, a::AbstractArray{T}) where {T}\n",
    "    # launch atomic reduction\n",
    "    a_atomic = copy(a) \n",
    "    b_atomic = CUDA.zeros(T, 1)\n",
    "\n",
    "    kernel_atomic = @cuda(\n",
    "        launch=false,\n",
    "        reduce_grid_atomic(+, a_atomic, b_atomic)\n",
    "    ) \n",
    "\n",
    "    config = launch_configuration(kernel_atomic.fun)\n",
    "    threads_config = min(config.threads, length(a))\n",
    "    threads = 1024\n",
    "    blocks = cld(length(a_atomic), threads*2)\n",
    "\n",
    "    @cuda(\n",
    "        threads=threads,\n",
    "        blocks=blocks,\n",
    "        reduce_grid_atomic(op, a_atomic, b_atomic)\n",
    "    ) \n",
    "    # launch shared memory  reduction\n",
    "    b_shared = CUDA.zeros(T, 1)\n",
    "\n",
    "    kernel_shared = @cuda(\n",
    "        launch=false,\n",
    "        reduce_grid_shared(+, a, b_shared)\n",
    "    ) \n",
    "\n",
    "    config = launch_configuration(kernel_shared.fun)\n",
    "    threads_config = min(config.threads, length(a))\n",
    "    threads = 1024\n",
    "    blocks = cld(length(a), threads*2)\n",
    "\n",
    "    @cuda(\n",
    "        threads=threads,\n",
    "        blocks=blocks,\n",
    "        reduce_grid_atomic(op, a, b_shared)\n",
    "    ) \n",
    "    # test outputs\n",
    "    @assert b_shared ≈ b_atomic\n",
    "\n",
    "    CUDA.@allowscalar b_atomic[]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf8517c",
   "metadata": {},
   "source": [
    "We just added the `@profile` macro and the `NVTX.@range`which makes it possible to enrich the profile tracer. Also to execute the function twice because sometimes an overhead is incurred at the first call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9d1027",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "main()\n",
    "\n",
    "function to launch the kernel\n",
    "\"\"\"\n",
    "function main()\n",
    "    N = 1024\n",
    "    c_a = rand(N, N, 10)\n",
    "    d_a = CuArray(c_a)\n",
    "    @test my_reduce(+, d_a) ≈ sum(c_a)\n",
    "\n",
    "    # profile it \n",
    "    CUDA.@profile begin \n",
    "        NVTX.@range \"my_reduce\" my_reduce(+, d_a)\n",
    "        NVTX.@range \"my_reduce\" my_reduce(+, d_a)\n",
    "    end\n",
    "end\n",
    "# execute the function\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a935ca",
   "metadata": {},
   "source": [
    "# NVIDIA Nsight Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958b4081",
   "metadata": {},
   "source": [
    "Then we open a bash terminal and execute:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee79613d",
   "metadata": {},
   "source": [
    "```bash\n",
    "nsys launch julia --trace=cuda  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1bad63",
   "metadata": {},
   "source": [
    "Then a Julia REPL is open and just include and automatically the report will be generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d408389b",
   "metadata": {},
   "source": [
    "<img src=nsys.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723da4f",
   "metadata": {},
   "source": [
    "#  NVIDIA Nsight Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf43a4",
   "metadata": {},
   "source": [
    "We open a bash terminal and execute:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4163cd54",
   "metadata": {},
   "source": [
    "```bash\n",
    "nv-nsight-cu-cli julia --trace=cuda  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62201bdf",
   "metadata": {},
   "source": [
    "When we close the app we obtain a lot of info, however by the moment is not possible to attach "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1094a32e",
   "metadata": {},
   "source": [
    "# Nvprof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1b2ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "We open a bash terminal and execute:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc0bb4",
   "metadata": {},
   "source": [
    "```bash\n",
    "nsys nvprof --profile-from-start off julia --trace=cuda \n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23af145e",
   "metadata": {},
   "source": [
    "When we close the app it is thrown: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7de29a",
   "metadata": {},
   "source": [
    "```\n",
    "NVTX Range Statistics:\n",
    "\n",
    " Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)   Style     Range  \n",
    " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  -------  ---------\n",
    "    100.0        6,822,643          2  3,411,321.5  3,411,321.5  3,322,959  3,499,684    124,963.4  PushPop  my_reduce\n",
    "\n",
    "[4/7] Executing 'cudaapisum' stats report\n",
    "\n",
    "CUDA API Statistics:\n",
    "\n",
    " Time (%)  Total Time (ns)  Num Calls  Avg (ns)   Med (ns)  Min (ns)  Max (ns)   StdDev (ns)               Name              \n",
    " --------  ---------------  ---------  ---------  --------  --------  ---------  -----------  -------------------------------\n",
    "     94.8      118,558,438        729  162,631.6  29,134.0     1,295  2,203,714    313,534.2  cuModuleUnload                 \n",
    "      4.5        5,648,389         12  470,699.1  27,428.0     4,843  2,714,353  1,046,947.5  cudaMemcpyAsync                \n",
    "      0.4          549,085          8   68,635.6   2,271.0     1,174    270,168    123,392.8  cuMemAllocAsync                \n",
    "      0.1           71,170         12    5,930.8   4,949.0     3,000     11,329      2,263.3  cudaLaunchKernel               \n",
    "      0.1           63,748         10    6,374.8   5,455.5     3,259     12,462      2,856.8  cuLaunchKernel                 \n",
    "      0.0           50,039          2   25,019.5  25,019.5    13,172     36,867     16,754.9  cuMemcpyDtoDAsync_v2           \n",
    "      0.0           44,053         13    3,388.7   1,121.0       804     14,373      4,606.7  cuMemFreeAsync                 \n",
    "      0.0           26,476          2   13,238.0  13,238.0    12,010     14,466      1,736.7  cuMemcpyDtoHAsync_v2           \n",
    "      0.0           18,916          6    3,152.7   2,340.0     1,510      7,240      2,185.8  cudaStreamSynchronize          \n",
    "      0.0           15,445          1   15,445.0  15,445.0    15,445     15,445          0.0  cuStreamDestroy_v2             \n",
    "      0.0           13,534          6    2,255.7   1,534.0       835      6,266      2,012.4  cudaEventQuery                 \n",
    "      0.0           11,679          2    5,839.5   5,839.5     5,740      5,939        140.7  cuCtxSynchronize               \n",
    "      0.0           10,035          6    1,672.5   1,376.0     1,007      3,405        900.4  cudaEventRecord                \n",
    "      0.0            9,558         12      796.5     653.0       466      1,662        424.2  cudaStreamGetCaptureInfo_v10010\n",
    "      0.0            2,465          2    1,232.5   1,232.5     1,087      1,378        205.8  cuStreamSynchronize            \n",
    "\n",
    "[5/7] Executing 'gpukernsum' stats report\n",
    "\n",
    "CUDA Kernel Statistics:\n",
    "\n",
    " Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                                                  Name                                                \n",
    " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ----------------------------------------------------------------------------------------------------\n",
    "     55.5        2,620,058          2  1,310,029.0  1,310,029.0  1,309,965  1,310,093         90.5  julia_reduce_grid_shared_3799(__, CuDeviceArray<Float64, (int)3, (int)1>, CuDeviceArray<Float64, (i…\n",
    "     39.8        1,877,861          2    938,930.5    938,930.5    931,122    946,739     11,042.9  julia_reduce_grid_atomic_3457(__, CuDeviceArray<Float64, (int)3, (int)1>, CuDeviceArray<Float64, (i…\n",
    "      4.4          208,700         12     17,391.7     18,847.5      9,216     19,968      3,765.3  void nrm2_kernel<double, double, double, (int)0, (int)0, (int)128>(cublasNrm2Params<T1, T3>)        \n",
    "      0.2           10,815          4      2,703.8      2,703.5      2,688      2,720         18.2  julia__5_2412(CuKernelContext, CuDeviceArray<Float64, (int)1, (int)1>, Float64)                     \n",
    "      0.1            6,367          2      3,183.5      3,183.5      3,136      3,231         67.2  julia_broadcast_kernel_3894(CuKernelContext, CuDeviceArray<Float64, (int)1, (int)1>, Broadcasted<Cu…\n",
    "\n",
    "[6/7] Executing 'gpumemtimesum' stats report\n",
    "\n",
    "CUDA Memory Operation Statistics (by time):\n",
    "\n",
    " Time (%)  Total Time (ns)  Count  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)      Operation     \n",
    " --------  ---------------  -----  ---------  ---------  --------  --------  -----------  ------------------\n",
    "     98.6        1,070,065      2  535,032.5  535,032.5   534,360   535,705        951.1  [CUDA memcpy DtoD]\n",
    "      0.9            9,473      8    1,184.1    1,120.5     1,088     1,408        130.2  [CUDA memcpy DtoH]\n",
    "      0.5            5,536      6      922.7      912.0       896       960         31.5  [CUDA memcpy HtoD]\n",
    "\n",
    "[7/7] Executing 'gpumemsizesum' stats report\n",
    "\n",
    "CUDA Memory Operation Statistics (by size):\n",
    "\n",
    " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n",
    " ----------  -----  --------  --------  --------  --------  -----------  ------------------\n",
    "    167.772      2    83.886    83.886    83.886    83.886        0.000  [CUDA memcpy DtoD]\n",
    "      0.000      6     0.000     0.000     0.000     0.000        0.000  [CUDA memcpy HtoD]\n",
    "      0.000      8     0.000     0.000     0.000     0.000        0.000  [CUDA memcpy DtoH]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
