{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea9838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a custom stylesheet in IJulia\n",
    "file = open(\"./../style.css\") # A .css file in the same folder as this notebook file\n",
    "styl = read(file, String) # Read the file\n",
    "HTML(\"$styl\") # Output as HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e36910",
   "metadata": {},
   "source": [
    "## CUDA.jl (based on [CUDA.jl/ docs](https://cuda.juliagpu.org/stable/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc99be72",
   "metadata": {},
   "source": [
    "<h2>In this notebook</h2>\n",
    "\n",
    "- [Set up](#Set-up)\n",
    "- [Product simple example](#Really-simple-example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b74d17c",
   "metadata": {},
   "source": [
    "# Set up\n",
    "\n",
    "The Julia CUDA works with NVIDIA driver however we don't need to install the entire CUDA toolkit, this will be automatically done just adding CUDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cf42dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.7/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.7/Manifest.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA toolkit 11.7, artifact installation\n",
      "NVIDIA driver 515.48.7, for CUDA 11.7\n",
      "CUDA driver 11.7\n",
      "\n",
      "Libraries: \n",
      "- CUBLAS: 11.10.1\n",
      "- CURAND: 10.2.10\n",
      "- CUFFT: 10.7.2\n",
      "- CUSOLVER: 11.3.5\n",
      "- CUSPARSE: 11.7.3\n",
      "- CUPTI: 17.0.0\n",
      "- NVML: 11.0.0+515.48.7\n",
      "- CUDNN: 8.30.2 (for CUDA 11.5.0)\n",
      "- CUTENSOR: 1.4.0 (for CUDA 11.5.0)\n",
      "\n",
      "Toolchain:\n",
      "- Julia: 1.7.3\n",
      "- LLVM: 12.0.1\n",
      "- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0\n",
      "- Device capability support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80\n",
      "\n",
      "1 device:\n",
      "  0: NVIDIA GeForce RTX 3060 Laptop GPU (sm_86, 5.687 GiB / 6.000 GiB available)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m     Testing\u001b[22m\u001b[39m CUDA\n",
      "\u001b[32m\u001b[1m      Status\u001b[22m\u001b[39m `/tmp/jl_RTiu1V/Project.toml`\n",
      " \u001b[90m [79e6a3ab] \u001b[39mAdapt v3.3.3\n",
      " \u001b[90m [ab4f0b2a] \u001b[39mBFloat16s v0.2.0\n",
      " \u001b[90m [052768ef] \u001b[39mCUDA v3.11.0\n",
      " \u001b[90m [864edb3b] \u001b[39mDataStructures v0.18.13\n",
      " \u001b[90m [7a1cc6ca] \u001b[39mFFTW v1.4.6\n",
      " \u001b[90m [0c68f7d7] \u001b[39mGPUArrays v8.3.2\n",
      " \u001b[90m [a98d9a8b] \u001b[39mInterpolations v0.13.6\n",
      " \u001b[90m [872c559c] \u001b[39mNNlib v0.8.8\n",
      " \u001b[90m [276daf66] \u001b[39mSpecialFunctions v2.1.6\n",
      " \u001b[90m [a759f4b9] \u001b[39mTimerOutputs v0.5.20\n",
      " \u001b[90m [ade2ca70] \u001b[39mDates `@stdlib/Dates`\n",
      " \u001b[90m [8ba89e20] \u001b[39mDistributed `@stdlib/Distributed`\n",
      " \u001b[90m [37e2e46d] \u001b[39mLinearAlgebra `@stdlib/LinearAlgebra`\n",
      " \u001b[90m [de0858da] \u001b[39mPrintf `@stdlib/Printf`\n",
      " \u001b[90m [3fa0cd96] \u001b[39mREPL `@stdlib/REPL`\n",
      " \u001b[90m [9a3f8284] \u001b[39mRandom `@stdlib/Random`\n",
      " \u001b[90m [2f01184e] \u001b[39mSparseArrays `@stdlib/SparseArrays`\n",
      " \u001b[90m [10745b16] \u001b[39mStatistics `@stdlib/Statistics`\n",
      " \u001b[90m [8dfed614] \u001b[39mTest `@stdlib/Test`\n",
      "\u001b[32m\u001b[1m      Status\u001b[22m\u001b[39m `/tmp/jl_RTiu1V/Manifest.toml`\n",
      " \u001b[90m [621f4979] \u001b[39mAbstractFFTs v1.1.0\n",
      " \u001b[90m [79e6a3ab] \u001b[39mAdapt v3.3.3\n",
      " \u001b[90m [13072b0f] \u001b[39mAxisAlgorithms v1.0.1\n",
      " \u001b[90m [ab4f0b2a] \u001b[39mBFloat16s v0.2.0\n",
      " \u001b[90m [fa961155] \u001b[39mCEnum v0.4.2\n",
      " \u001b[90m [052768ef] \u001b[39mCUDA v3.11.0\n",
      " \u001b[90m [d360d2e6] \u001b[39mChainRulesCore v1.15.0\n",
      " \u001b[90m [9e997f8a] \u001b[39mChangesOfVariables v0.1.3\n",
      " \u001b[90m [34da2185] \u001b[39mCompat v3.45.0\n",
      " \u001b[90m [864edb3b] \u001b[39mDataStructures v0.18.13\n",
      " \u001b[90m [ffbed154] \u001b[39mDocStringExtensions v0.8.6\n",
      " \u001b[90m [e2ba6199] \u001b[39mExprTools v0.1.8\n",
      " \u001b[90m [7a1cc6ca] \u001b[39mFFTW v1.4.6\n",
      " \u001b[90m [0c68f7d7] \u001b[39mGPUArrays v8.3.2\n",
      " \u001b[90m [61eb1bfa] \u001b[39mGPUCompiler v0.16.0\n",
      " \u001b[90m [a98d9a8b] \u001b[39mInterpolations v0.13.6\n",
      " \u001b[90m [3587e190] \u001b[39mInverseFunctions v0.1.7\n",
      " \u001b[90m [92d709cd] \u001b[39mIrrationalConstants v0.1.1\n",
      " \u001b[90m [692b3bcd] \u001b[39mJLLWrappers v1.4.1\n",
      " \u001b[90m [929cbde3] \u001b[39mLLVM v4.14.0\n",
      " \u001b[90m [2ab3a3ac] \u001b[39mLogExpFunctions v0.3.15\n",
      " \u001b[90m [872c559c] \u001b[39mNNlib v0.8.8\n",
      " \u001b[90m [6fe1bfb0] \u001b[39mOffsetArrays v1.12.6\n",
      " \u001b[90m [bac558e1] \u001b[39mOrderedCollections v1.4.1\n",
      " \u001b[90m [21216c6a] \u001b[39mPreferences v1.3.0\n",
      " \u001b[90m [74087812] \u001b[39mRandom123 v1.5.0\n",
      " \u001b[90m [e6cf234a] \u001b[39mRandomNumbers v1.5.3\n",
      " \u001b[90m [c84ed2f1] \u001b[39mRatios v0.4.3\n",
      " \u001b[90m [189a3867] \u001b[39mReexport v1.2.2\n",
      " \u001b[90m [ae029012] \u001b[39mRequires v1.3.0\n",
      " \u001b[90m [276daf66] \u001b[39mSpecialFunctions v2.1.6\n",
      " \u001b[90m [90137ffa] \u001b[39mStaticArrays v1.4.7\n",
      " \u001b[90m [a759f4b9] \u001b[39mTimerOutputs v0.5.20\n",
      " \u001b[90m [efce3f68] \u001b[39mWoodburyMatrices v0.5.5\n",
      " \u001b[90m [f5851436] \u001b[39mFFTW_jll v3.3.10+0\n",
      " \u001b[90m [1d5cc7b8] \u001b[39mIntelOpenMP_jll v2018.0.3+2\n",
      " \u001b[90m [dad2f222] \u001b[39mLLVMExtra_jll v0.0.16+0\n",
      " \u001b[90m [856f044c] \u001b[39mMKL_jll v2022.0.0+0\n",
      " \u001b[90m [efe28fd5] \u001b[39mOpenSpecFun_jll v0.5.5+0\n",
      " \u001b[90m [0dad84c5] \u001b[39mArgTools `@stdlib/ArgTools`\n",
      " \u001b[90m [56f22d72] \u001b[39mArtifacts `@stdlib/Artifacts`\n",
      " \u001b[90m [2a0f44e3] \u001b[39mBase64 `@stdlib/Base64`\n",
      " \u001b[90m [ade2ca70] \u001b[39mDates `@stdlib/Dates`\n",
      " \u001b[90m [8bb1440f] \u001b[39mDelimitedFiles `@stdlib/DelimitedFiles`\n",
      " \u001b[90m [8ba89e20] \u001b[39mDistributed `@stdlib/Distributed`\n",
      " \u001b[90m [f43a241f] \u001b[39mDownloads `@stdlib/Downloads`\n",
      " \u001b[90m [7b1f6079] \u001b[39mFileWatching `@stdlib/FileWatching`\n",
      " \u001b[90m [b77e0a4c] \u001b[39mInteractiveUtils `@stdlib/InteractiveUtils`\n",
      " \u001b[90m [4af54fe1] \u001b[39mLazyArtifacts `@stdlib/LazyArtifacts`\n",
      " \u001b[90m [b27032c2] \u001b[39mLibCURL `@stdlib/LibCURL`\n",
      " \u001b[90m [76f85450] \u001b[39mLibGit2 `@stdlib/LibGit2`\n",
      " \u001b[90m [8f399da3] \u001b[39mLibdl `@stdlib/Libdl`\n",
      " \u001b[90m [37e2e46d] \u001b[39mLinearAlgebra `@stdlib/LinearAlgebra`\n",
      " \u001b[90m [56ddb016] \u001b[39mLogging `@stdlib/Logging`\n",
      " \u001b[90m [d6f4376e] \u001b[39mMarkdown `@stdlib/Markdown`\n",
      " \u001b[90m [a63ad114] \u001b[39mMmap `@stdlib/Mmap`\n",
      " \u001b[90m [ca575930] \u001b[39mNetworkOptions `@stdlib/NetworkOptions`\n",
      " \u001b[90m [44cfe95a] \u001b[39mPkg `@stdlib/Pkg`\n",
      " \u001b[90m [de0858da] \u001b[39mPrintf `@stdlib/Printf`\n",
      " \u001b[90m [3fa0cd96] \u001b[39mREPL `@stdlib/REPL`\n",
      " \u001b[90m [9a3f8284] \u001b[39mRandom `@stdlib/Random`\n",
      " \u001b[90m [ea8e919c] \u001b[39mSHA `@stdlib/SHA`\n",
      " \u001b[90m [9e88b42a] \u001b[39mSerialization `@stdlib/Serialization`\n",
      " \u001b[90m [1a1011a3] \u001b[39mSharedArrays `@stdlib/SharedArrays`\n",
      " \u001b[90m [6462fe0b] \u001b[39mSockets `@stdlib/Sockets`\n",
      " \u001b[90m [2f01184e] \u001b[39mSparseArrays `@stdlib/SparseArrays`\n",
      " \u001b[90m [10745b16] \u001b[39mStatistics `@stdlib/Statistics`\n",
      " \u001b[90m [fa267f1f] \u001b[39mTOML `@stdlib/TOML`\n",
      " \u001b[90m [a4e569a6] \u001b[39mTar `@stdlib/Tar`\n",
      " \u001b[90m [8dfed614] \u001b[39mTest `@stdlib/Test`\n",
      " \u001b[90m [cf7118a7] \u001b[39mUUIDs `@stdlib/UUIDs`\n",
      " \u001b[90m [4ec0a83e] \u001b[39mUnicode `@stdlib/Unicode`\n",
      " \u001b[90m [e66e0078] \u001b[39mCompilerSupportLibraries_jll `@stdlib/CompilerSupportLibraries_jll`\n",
      " \u001b[90m [deac9b47] \u001b[39mLibCURL_jll `@stdlib/LibCURL_jll`\n",
      " \u001b[90m [29816b5a] \u001b[39mLibSSH2_jll `@stdlib/LibSSH2_jll`\n",
      " \u001b[90m [c8ffd9c3] \u001b[39mMbedTLS_jll `@stdlib/MbedTLS_jll`\n",
      " \u001b[90m [14a3606d] \u001b[39mMozillaCACerts_jll `@stdlib/MozillaCACerts_jll`\n",
      " \u001b[90m [4536629a] \u001b[39mOpenBLAS_jll `@stdlib/OpenBLAS_jll`\n",
      " \u001b[90m [05823500] \u001b[39mOpenLibm_jll `@stdlib/OpenLibm_jll`\n",
      " \u001b[90m [83775a58] \u001b[39mZlib_jll `@stdlib/Zlib_jll`\n",
      " \u001b[90m [8e850b90] \u001b[39mlibblastrampoline_jll `@stdlib/libblastrampoline_jll`\n",
      " \u001b[90m [8e850ede] \u001b[39mnghttp2_jll `@stdlib/nghttp2_jll`\n",
      " \u001b[90m [3f19e933] \u001b[39mp7zip_jll `@stdlib/p7zip_jll`\n",
      "\u001b[32m\u001b[1m     Testing\u001b[22m\u001b[39m Running tests...\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYou are running the CUDA.jl test suite with only a single thread; this will take a long time.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mConsider launching Julia with `--threads auto` to run tests in parallel.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Main ~/.julia/packages/CUDA/tTK8Y/test/runtests.jl:52\u001b[39m\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSystem information:\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mCUDA toolkit 11.7, artifact installation\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mNVIDIA driver 515.48.7, for CUDA 11.7\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mCUDA driver 11.7\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mLibraries: \n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- CUBLAS: 11.10.1\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- CURAND: 10.2.10\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- CUFFT: 10.7.2\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- CUSOLVER: 11.3.5\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- CUSPARSE: 11.7.3\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- CUPTI: 17.0.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- NVML: 11.0.0+515.48.7\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- CUDNN: 8.30.2 (for CUDA 11.5.0)\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- CUTENSOR: 1.4.0 (for CUDA 11.5.0)\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mToolchain:\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- Julia: 1.7.3\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- LLVM: 12.0.1\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- Device capability support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m1 device:\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  0: NVIDIA GeForce RTX 3060 Laptop GPU (sm_86, 5.687 GiB / 6.000 GiB available)\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTesting using 1 device(s): 0. NVIDIA GeForce RTX 3060 Laptop GPU (UUID 04dfd8eb-c8ee-265f-a1f8-6a0e5f7719f9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m                                                  | \u001b[37m         | ---------------- GPU ---------------- | ---------------- CPU ---------------- |\u001b[39m\n",
      "\u001b[37mTest\u001b[39m\u001b[37m                                     (Worker) | \u001b[39m\u001b[37mTime (s) | GC (s) | GC % | Alloc (MB) | RSS (MB) | GC (s) | GC % | Alloc (MB) | RSS (MB) |\u001b[39m\n",
      "\u001b[37minitialization\u001b[39m\u001b[37m                                (2) | \u001b[39m\u001b[37m    5.98 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m  137.00 | \u001b[39m\u001b[37m  0.10 | \u001b[39m\u001b[37m 1.7 | \u001b[39m\u001b[37m    508.95 | \u001b[39m\u001b[37m  897.57 |\u001b[39m\n",
      "\u001b[37mgpuarrays/indexing scalar\u001b[39m\u001b[37m                     (2) | \u001b[39m\u001b[37m   28.52 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.01 | \u001b[39m\u001b[37m  149.00 | \u001b[39m\u001b[37m  1.57 | \u001b[39m\u001b[37m 5.5 | \u001b[39m\u001b[37m   4711.30 | \u001b[39m\u001b[37m  897.57 |\u001b[39m\n",
      "\u001b[37mgpuarrays/reductions/reducedim!\u001b[39m\u001b[37m               (2) | \u001b[39m\u001b[37m  102.31 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      1.03 | \u001b[39m\u001b[37m  151.00 | \u001b[39m\u001b[37m 10.07 | \u001b[39m\u001b[37m 9.8 | \u001b[39m\u001b[37m  22640.66 | \u001b[39m\u001b[37m 1104.06 |\u001b[39m\n",
      "\u001b[37mgpuarrays/linalg\u001b[39m\u001b[37m                              (2) | \u001b[39m\u001b[37m   53.61 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      9.47 | \u001b[39m\u001b[37m  519.00 | \u001b[39m\u001b[37m  3.55 | \u001b[39m\u001b[37m 6.6 | \u001b[39m\u001b[37m   9438.65 | \u001b[39m\u001b[37m 2255.43 |\u001b[39m\n",
      "\u001b[37mgpuarrays/math/power\u001b[39m\u001b[37m                          (2) | \u001b[39m\u001b[37m   38.72 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.01 | \u001b[39m\u001b[37m  519.00 | \u001b[39m\u001b[37m  3.51 | \u001b[39m\u001b[37m 9.1 | \u001b[39m\u001b[37m   8090.35 | \u001b[39m\u001b[37m 2468.77 |\u001b[39m\n",
      "\u001b[37mgpuarrays/linalg/mul!/vector-matrix\u001b[39m\u001b[37m           (2) | \u001b[39m\u001b[37m   56.72 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.02 | \u001b[39m\u001b[37m  519.00 | \u001b[39m\u001b[37m  3.87 | \u001b[39m\u001b[37m 6.8 | \u001b[39m\u001b[37m  11644.80 | \u001b[39m\u001b[37m 2592.62 |\u001b[39m\n",
      "\u001b[37mgpuarrays/indexing multidimensional\u001b[39m\u001b[37m           (2) | \u001b[39m\u001b[37m   38.94 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      1.21 | \u001b[39m\u001b[37m  151.00 | \u001b[39m\u001b[37m  2.62 | \u001b[39m\u001b[37m 6.7 | \u001b[39m\u001b[37m   7953.38 | \u001b[39m\u001b[37m 2673.44 |\u001b[39m\n",
      "\u001b[37mgpuarrays/interface\u001b[39m\u001b[37m                           (2) | \u001b[39m\u001b[37m    4.47 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m  149.00 | \u001b[39m\u001b[37m  0.28 | \u001b[39m\u001b[37m 6.2 | \u001b[39m\u001b[37m    859.97 | \u001b[39m\u001b[37m 2673.44 |\u001b[39m\n",
      "\u001b[37mgpuarrays/reductions/any all count\u001b[39m\u001b[37m            (2) | \u001b[39m\u001b[37m   16.02 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m  149.00 | \u001b[39m\u001b[37m  1.51 | \u001b[39m\u001b[37m 9.4 | \u001b[39m\u001b[37m   3978.25 | \u001b[39m\u001b[37m 2673.44 |\u001b[39m\n",
      "\u001b[37mgpuarrays/reductions/minimum maximum extrema\u001b[39m\u001b[37m  (2) | \u001b[39m\u001b[37m  150.85 | \u001b[39m\u001b[37m  0.01 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      1.41 | \u001b[39m\u001b[37m  153.00 | \u001b[39m\u001b[37m 11.15 | \u001b[39m\u001b[37m 7.4 | \u001b[39m\u001b[37m  33427.39 | \u001b[39m\u001b[37m 2951.73 |\u001b[39m\n",
      "\u001b[37mgpuarrays/uniformscaling\u001b[39m\u001b[37m                      (2) | \u001b[39m\u001b[37m    9.34 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.01 | \u001b[39m\u001b[37m  149.00 | \u001b[39m\u001b[37m  0.47 | \u001b[39m\u001b[37m 5.0 | \u001b[39m\u001b[37m   1296.96 | \u001b[39m\u001b[37m 2951.73 |\u001b[39m\n",
      "\u001b[37mgpuarrays/linalg/mul!/matrix-matrix\u001b[39m\u001b[37m           (2) | \u001b[39m\u001b[37m  105.18 | \u001b[39m\u001b[37m  0.01 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.12 | \u001b[39m\u001b[37m  521.00 | \u001b[39m\u001b[37m  6.55 | \u001b[39m\u001b[37m 6.2 | \u001b[39m\u001b[37m  19190.33 | \u001b[39m\u001b[37m 4041.64 |\u001b[39m\n",
      "\u001b[37mgpuarrays/math/intrinsics\u001b[39m\u001b[37m                     (2) | \u001b[39m\u001b[37m    3.84 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m  149.00 | \u001b[39m\u001b[37m  0.21 | \u001b[39m\u001b[37m 5.4 | \u001b[39m\u001b[37m    712.81 | \u001b[39m\u001b[37m 4041.69 |\u001b[39m\n",
      "\u001b[37mgpuarrays/linalg/norm\u001b[39m\u001b[37m                         (2) | \u001b[39m\u001b[37m  259.01 | \u001b[39m\u001b[37m  0.01 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.02 | \u001b[39m\u001b[37m  225.00 | \u001b[39m\u001b[37m 21.84 | \u001b[39m\u001b[37m 8.4 | \u001b[39m\u001b[37m  47397.45 | \u001b[39m\u001b[37m 5273.32 |\u001b[39m\n",
      "\u001b[37mgpuarrays/statistics\u001b[39m\u001b[37m                          (2) | \u001b[39m\u001b[37m   86.51 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      1.51 | \u001b[39m\u001b[37m  519.00 | \u001b[39m\u001b[37m  4.88 | \u001b[39m\u001b[37m 5.6 | \u001b[39m\u001b[37m  13949.86 | \u001b[39m\u001b[37m 6343.45 |\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# Run julia using the flag \n",
    "```bash\n",
    "julia --threads auto \n",
    "```\n",
    "\n",
    "# Install the pkg \n",
    "import Pkg; \n",
    "Pkg.add(\"CUDA\")\n",
    "\n",
    "# get the tool version \n",
    "import CUDA \n",
    "CUDA.versioninfo()\n",
    "\n",
    "# test pkg\n",
    "Pkg.test(\"CUDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b7bd7",
   "metadata": {},
   "source": [
    "# Really simple example\n",
    "\n",
    "In this first simple example we test the main differences on CPU and GPU implementation of a a multiply operation. Let's start with CPU implementation and creating a test problem for large array \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f993102",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2^22 # size of both vec\n",
    "x = fill(1.0f0, N) # x vec\n",
    "y = fill(2.0f0, N) # y vec\n",
    "\n",
    "# result and test\n",
    "r = x.*y\n",
    "\n",
    "using Test \n",
    "@test (all(r.==x[1]*y[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bad365",
   "metadata": {},
   "source": [
    "```julia\n",
    "Test Passed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c878e94c",
   "metadata": {},
   "source": [
    "Let's know implemente a CPU paralalization with serial `serial_cpu_multiply` and `parallel_cpu_multiply` function : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd98056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the number of cpu threads \n",
    "JULIA_NUM_THREADS = 6\n",
    "\n",
    "# Declare parallel function cpu\n",
    "function serial_cpu_multiply(x,y)\n",
    "    for i in eachindex(x,y)\n",
    "        @inbounds r[i] = x[i]*y[i]\n",
    "    end\n",
    "    return r \n",
    "end\n",
    "\n",
    "\n",
    "# Declare parallel function cpu\n",
    "function parallel_cpu_multiply(x,y)\n",
    "    Threads.@threads for i in eachindex(x,y)\n",
    "        @inbounds r[i] = x[i]*y[i]\n",
    "    end\n",
    "    return r \n",
    "end\n",
    "\n",
    "# Execute function for y and x vec\n",
    "r_serial_cpu = serial_cpu_multiply(x,y)\n",
    "r_parallel_cpu = parallel_cpu_multiply(x,y)\n",
    "\n",
    "# Run test \n",
    "\n",
    "@test (all(r_parallel_cpu.==(x[1]*y[1])) && all(r_serial_cpu.==(x[1]*y[1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f476652",
   "metadata": {},
   "source": [
    "```julia\n",
    "Test Passed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e8613",
   "metadata": {},
   "source": [
    "Let's now measure the exution time with `BenchmarkTools` pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1372202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and install pkg\n",
    "#Pkg.add(\"BenchmarkTools\")\n",
    "\n",
    "# Let's use it \n",
    "using BenchmarkTools\n",
    "\n",
    "# Serial and parallel cpu multiply\n",
    "@btime serial_cpu_multiply(x,y)\n",
    "@btime parallel_cpu_multiply(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71351455",
   "metadata": {},
   "source": [
    "```julia\n",
    "68.364 ms (8388097 allocations: 127.99 MiB)\n",
    "```\n",
    "and \n",
    "```julia\n",
    "26.242 ms (8388138 allocations: 128.00 MiB)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea332a1f",
   "metadata": {},
   "source": [
    "Let's now implement it using GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641be1ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load CUDA \n",
    "using CUDA \n",
    "\n",
    "# define a vecotr on the GPU \n",
    "x_d = CUDA.fill(1.0f0, N)\n",
    "y_d = CUDA.fill(2.0f0, N)\n",
    "r_d = CUDA.fill(0.0f0, N)\n",
    "\n",
    "\n",
    "# define the function \n",
    "function multiply_gpu(y,x)\n",
    "    CUDA.@sync begin \n",
    "        return y.*x\n",
    "    end\n",
    "end\n",
    "\n",
    "# exute and measure time\n",
    "@btime multiply_gpu(x_d, y_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b26444",
   "metadata": {},
   "source": [
    "We obtain a much faster implementation! \n",
    "```julia\n",
    "3.427 μs (40 allocations: 2.83 KiB)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ae494a",
   "metadata": {},
   "source": [
    "The `@sync` macro is the interesting thing here. This will force the CPU to wait untill the GPU ends up its work and at that point will continue. But most of the time you don't need to synchronize explicitly: many operations, like copying memory from the GPU to the CPU, implicitly synchronize execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ab8b39",
   "metadata": {},
   "source": [
    "This way to perform high level computation is okay but we need to dive into to perform specific stuff under the hood. Let's implement our kernel to do that task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc4464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create my kernel\n",
    "function multiply_gpu_kernel!(x, y, r)\n",
    "    for i in 1:length(x)\n",
    "         r[i] = x[i]*y[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "\n",
    "# execute the kerenel with autmatic number of threads and blocks \n",
    "@cuda multiply_gpu_kernel!(x_d, y_d, r_d)\n",
    "\n",
    "# lets create a benchmark function to test it \n",
    "function bench_multiply_gpu(x_d, y_d, r_d)\n",
    "    CUDA.@sync begin\n",
    "        @cuda multiply_gpu_kernel!(x_d, y_d, r_d)\n",
    "    end\n",
    "end\n",
    "\n",
    "@btime bench_multiply_gpu(x_d, y_d, r_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64406efa",
   "metadata": {},
   "source": [
    "```julia\n",
    "435.169 ms (61 allocations: 3.89 KiB)\n",
    "```\n",
    "Thats a really slower version than the other implementation, what happen ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b968943",
   "metadata": {},
   "source": [
    "After using  `CuArrays` `x_d` and `y_d`, we can lunch our kernel launch via `@cuda`. The `@cuda` macro statement, it will compile the kernel `(bench_multiply_gpu!)` for execution on the GPU. Once compiled, future invocations are fast. You can see what `@cuda` expands to using `?@cuda` from the Julia prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691e2fce",
   "metadata": {},
   "source": [
    "```\n",
    "cuda\n",
    "  @cuda [kwargs...] func(args...)\n",
    "\n",
    "  High-level interface for executing code on a GPU. The @cuda macro should\n",
    "  prefix a call, with func a callable function or object that should return\n",
    "  nothing. It will be compiled to a CUDA function upon first use, and to a\n",
    "  certain extent arguments will be converted and managed automatically using\n",
    "  cudaconvert. Finally, a call to cudacall is performed, scheduling a kernel\n",
    "  launch on the current CUDA context.\n",
    "\n",
    "  Several keyword arguments are supported that influence the behavior of\n",
    "  @cuda.\n",
    "\n",
    "    •  launch: whether to launch this kernel, defaults to true. If\n",
    "       false the returned kernel object should be launched by calling\n",
    "       it and passing arguments again.\n",
    "\n",
    "    •  dynamic: use dynamic parallelism to launch device-side kernels,\n",
    "       defaults to false.\n",
    "\n",
    "    •  arguments that influence kernel compilation: see cufunction and\n",
    "       dynamic_cufunction\n",
    "\n",
    "    •  arguments that influence kernel launch: see CUDA.HostKernel and\n",
    "       CUDA.DeviceKernel\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4aec84",
   "metadata": {},
   "source": [
    "# Profiling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e53664",
   "metadata": {},
   "source": [
    "Often is really important obtain a profiling for our GPU program, to check coalsceed access, race conditions problems, memory managment acces, etc. For that, we can call `nvprof` tool from NVIDIA. On a Unix system we should execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be9d8bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "$ nvprof --profile-from-start off /path/to/julia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f22aec",
   "metadata": {},
   "source": [
    "The `/path/to/julia` is the path to julia binary. Note that we don't initialize immediately the profiler but we can call the CUDA API's with the macro @profile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0861c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA.@profile bench_multiply_gpu(x_d, y_d, r_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dbced5",
   "metadata": {},
   "source": [
    "But nvprof is not longer used for GPUs with compute capalities newer than 7.0, instead we need nsys (Nsight system), to set nsys to julia we run: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01acaeb4",
   "metadata": {},
   "source": [
    "```bash\n",
    "nsys launch julia --threads auto  \n",
    "```\n",
    "then we can execute the previous code but adding the macro CUDA.@profile before we lunch the kernel:\n",
    "```julia \n",
    "julia> using CUDA\n",
    "\n",
    "julia> a = CUDA.rand(1024,1024,1024);\n",
    "\n",
    "julia> sin.(a);\n",
    "\n",
    "julia> CUDA.@profile sin.(a);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd239400",
   "metadata": {},
   "source": [
    "Then we open Nshight System using nsys-cli "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8673ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
