{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17dfa7d6",
   "metadata": {},
   "source": [
    "## Setting up a custom stylesheet in IJulia\n",
    "file = open(\"./../style.css\") # A .css file in the same folder as this notebook file\n",
    "styl = read(file, String) # Read the file\n",
    "HTML(\"$styl\") # Output as HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e590de1c",
   "metadata": {},
   "source": [
    "## juliaGPU (based on the [juliaGPU 2021 workshop](https://www.youtube.com/watch?v=Hz9IMJuW5hU))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e398e",
   "metadata": {},
   "source": [
    "We have different packages available at juliaGPU organization [gitHub](https://cuda.juliagpu.org/stable/) but only some of them are useful:\n",
    "\n",
    "* Infrastructure: `GPUArrays.jl` `Adapt.jl` `GPUComplier.jl`\n",
    "* Library wrappers: `OpenCL.jl` `VulkanCore.jl` `MetalCore.jl`\n",
    "* Programming frameworks (useful): `CUDA.jl`, `AMDGPU.jl`,`oneAPI.jl` and `ArrayFire.jl`(C library)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb872bce",
   "metadata": {},
   "source": [
    "### Back end \n",
    "Analogsly to Julia with `LLVM.jl` a real time complier that generates machine code into the CPU they use `GPUComplier.jl`, then `LLVM.jl` create `LLVM.jl` to generate GPU code.  \n",
    "### Architecture \n",
    "From Host to Device the data is transferred via PCI/Express bus which is actually really slow, thus we want to avoid copying data from CPU to GPU. Then the CPU inside has a lot of CPU SM streaming multi-processors that can access to shared mem (similar to cahce L2), registers (cache L1) or global memory. Occupancy is the Nvidia term of how to manage all the memory available by a program. \n",
    "### Implicit parallelism\n",
    "There are inner functions that are already implemented using parallelism like `CUDA.reduce()`, and we don't have to worry about implementing it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad75554",
   "metadata": {},
   "source": [
    "<img src=arch.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc55a5ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA toolkit 11.7, artifact installation\n",
      "NVIDIA driver 515.57.0, for CUDA 11.7\n",
      "CUDA driver 11.7\n",
      "\n",
      "Libraries: \n",
      "- CUBLAS: 11.10.1\n",
      "- CURAND: 10.2.10\n",
      "- CUFFT: 10.7.2\n",
      "- CUSOLVER: 11.3.5\n",
      "- CUSPARSE: 11.7.3\n",
      "- CUPTI: 17.0.0\n",
      "- NVML: 11.0.0+515.57\n",
      "- CUDNN: 8.30.2 (for CUDA 11.5.0)\n",
      "- CUTENSOR: 1.4.0 (for CUDA 11.5.0)\n",
      "\n",
      "Toolchain:\n",
      "- Julia: 1.7.3\n",
      "- LLVM: 12.0.1\n",
      "- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0\n",
      "- Device capability support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80\n",
      "\n",
      "1 device:\n",
      "  0: NVIDIA GeForce RTX 3060 Laptop GPU (sm_86, 5.382 GiB / 6.000 GiB available)\n"
     ]
    }
   ],
   "source": [
    "# load CUDA.jl\n",
    "import Pkg\n",
    "using CUDA\n",
    "\n",
    "# check out the CUDA version\n",
    "CUDA.versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc4a34a",
   "metadata": {},
   "source": [
    "### Options for CUDA selection\n",
    "There's two special environment variables that can currently be used to influence the CUDA toolkit that's used:\n",
    "\n",
    "* `JULIA_CUDA_VERSION` that sets the CUDA version that we want to install (this is useful if any update version is not working with our set up) \n",
    "* `JULIA_CUDA_USE_BINARYBUILDER` set to false will install \n",
    "\n",
    "Besides we have this customization variables is not recommended to change because CUDA.jl will automatically select the correct one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa6a6ff",
   "metadata": {},
   "source": [
    "### High order abstraction \n",
    "As CUBLAS or other libraries are C libraries are limited in terms of which types it supports so we need a generic fallbacks. Although the CuArray type and its support for NVIDIA's vendor libraries is nice, it isn't really novel and could as well have been implemented in Python (and libraries like CuPy do exactly that). The real flexibility of Julia's arrays comes from its higher-order abstractions, where you combine the abstraction with custom code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80458fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = CUDA.ones(5)\n",
    "# aply a function to each element of a\n",
    "y = broadcast(a) do x\n",
    "    2x\n",
    "end\n",
    "# or what is the same \n",
    "y = a.*2\n",
    "# we have map \n",
    "y = map(a) do x\n",
    "    2x\n",
    "end\n",
    "# also accumulate and reduce \n",
    "red = reduce(+, a)\n",
    "acc = accumulate(*,a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a520c40",
   "metadata": {},
   "source": [
    "Here it is important to highlight that CUDA.jl doesn't provides a tensor complier, as consequence we can't merge functions like accumulate(reduce) for instance. If we merge operations then will be launching as many kernels as nesting operations are executed. For instance what happen if we want to sum each column of a CuArray and divide into the number of elements of each column matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b183117",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m150.605 μs\u001b[22m\u001b[39m … \u001b[35m 43.825 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 36.75%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m161.694 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m198.991 μs\u001b[22m\u001b[39m ± \u001b[32m876.397 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m3.75% ±  0.86%\n",
       "\n",
       "  \u001b[39m▇\u001b[39m█\u001b[34m▆\u001b[39m\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[32m▃\u001b[39m\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\n",
       "  \u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▆\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m█\u001b[39m█\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▇\u001b[39m▅\u001b[39m▄\u001b[39m▁\u001b[39m▄\u001b[39m▃\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▄\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▅\u001b[39m▄\u001b[39m▃\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▅\u001b[39m▄\u001b[39m \u001b[39m█\n",
       "  151 μs\u001b[90m        \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m        561 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m23.39 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m501\u001b[39m."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = CUDA.ones(10,10)\n",
    "\n",
    "y = broadcast(eachcol(a)) do x\n",
    "    sum(x)/length(eachcol(a))\n",
    "end\n",
    "# this will create 10 kernels one per each column of a \n",
    "# let's mesure the performance \n",
    "using BenchmarkTools \n",
    "@benchmark CUDA.@sync y = broadcast(eachcol(a)) do x\n",
    "                        sum(x)/length(eachcol(a))\n",
    "                      end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6416b55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m12.377 μs\u001b[22m\u001b[39m … \u001b[35m 1.345 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m14.223 μs              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m16.793 μs\u001b[22m\u001b[39m ± \u001b[32m20.482 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m▂\u001b[39m▇\u001b[39m█\u001b[39m▇\u001b[39m▆\u001b[34m▆\u001b[39m\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[32m▃\u001b[39m\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m▁\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\n",
       "  \u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m█\u001b[39m▆\u001b[39m▆\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m \u001b[39m█\n",
       "  12.4 μs\u001b[90m      \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m      35.8 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m2.70 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m57\u001b[39m."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another possiblity is to use dims keyword like\n",
    "sum(a; dims=2) / size(a,2)\n",
    "\n",
    "using BenchmarkTools \n",
    "@benchmark CUDA.@sync sum(a; dims=2) / size(a,2)\n",
    "# is arround 10 times faster since only one kernel is complied and executed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3918d4aa",
   "metadata": {},
   "source": [
    "### More complicated applications can be faced using Tulio.jl, a tensor compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4705b4f",
   "metadata": {},
   "source": [
    "### Reduce example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2da97a",
   "metadata": {},
   "source": [
    "We might start with a serial simple for implementation for one single thread;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b856c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: \u001b[90m#= In[5]:18 =#\u001b[39m CUDA.@allowscalar d_b[] == sum(c_a)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function reduce_singlethread(op, a, b)\n",
    "    for i in 1:length(a)\n",
    "        b[] = op(b[], a[i])\n",
    "    end\n",
    "    return\n",
    "end\n",
    "# define variables\n",
    "c_a = 1:16\n",
    "d_a = CuArray(1:16)\n",
    "d_b = CuArray([0])\n",
    "# lunch kernel\n",
    "@cuda(\n",
    "    threads = 1,\n",
    "    reduce_singlethread(+, d_a, d_b)\n",
    "    )\n",
    "# test the result \n",
    "using Test\n",
    "@test CUDA.@allowscalar d_b[] == sum(c_a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87244b84",
   "metadata": {},
   "source": [
    "Obviously this not a very efficient implementation since is a serial one (only one thread). We can test it using `@sync` macro to force the CPU to wait until GPU completes its work. Otherwise we will be measuring the time to queue an operation and not the time it takes to finish it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "648341d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 17 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m311.845 ms\u001b[22m\u001b[39m … \u001b[35m312.306 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m312.103 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m312.117 ms\u001b[22m\u001b[39m ± \u001b[32m117.301 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m█\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m█\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m \u001b[39m█\u001b[39m \u001b[39m \u001b[39m█\u001b[39m \u001b[34m█\u001b[39m\u001b[39m█\u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m█\u001b[39m█\u001b[39m \u001b[39m \u001b[39m \u001b[39m█\u001b[39m█\u001b[39m \u001b[39m \u001b[39m█\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m█\u001b[39m \u001b[39m \u001b[39m█\u001b[39m \u001b[39m \u001b[39m█\u001b[39m \u001b[39m \n",
       "  \u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m▁\u001b[32m▁\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m \u001b[39m▁\n",
       "  312 ms\u001b[90m           Histogram: frequency by time\u001b[39m          312 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m3.42 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m62\u001b[39m."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 2048\n",
    "@benchmark CUDA.@sync @cuda( \n",
    "    threads = 1, \n",
    "        reduce_singlethread(+,   $(CUDA.rand(N, N)), $(CUDA.rand(N, N)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded943f3",
   "metadata": {},
   "source": [
    "Other option is to use `CUDA.@atomic` to take advantage of the SMPD paradigm: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3785b68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: \u001b[90m#= In[7]:18 =#\u001b[39m CUDA.@allowscalar d_b[] == sum(c_a)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function reduce_atomic(op, a, b)\n",
    "    index = threadIdx().x + (blockIdx().x -1)*blockDim().x\n",
    "    @inbounds CUDA.@atomic b[] = b[] + a[index]\n",
    "    return\n",
    "end\n",
    "# define variables\n",
    "c_a = 1:16\n",
    "d_a = CuArray(1:16)\n",
    "d_b = CuArray([0])\n",
    "# lunch kernel\n",
    "@cuda(\n",
    "    threads = 16,\n",
    "    blocks = 1,\n",
    "    reduce_singlethread(+, d_a, d_b)\n",
    "    )\n",
    "# test the result \n",
    "using Test\n",
    "@test CUDA.@allowscalar d_b[] == sum(c_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dccffd3",
   "metadata": {},
   "source": [
    "Let's now benchmark our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7ea4e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 3 evaluations.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m8.049 μs\u001b[22m\u001b[39m … \u001b[35m143.825 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m8.783 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m9.774 μs\u001b[22m\u001b[39m ± \u001b[32m  4.894 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m▁\u001b[39m▅\u001b[39m▇\u001b[39m█\u001b[34m▇\u001b[39m\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[32m▄\u001b[39m\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m \u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\n",
       "  \u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▇\u001b[39m▇\u001b[39m▅\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▆\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▆\u001b[39m▆\u001b[39m \u001b[39m█\n",
       "  8.05 μs\u001b[90m      \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m      17.9 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m304 bytes\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m5\u001b[39m."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 2048\n",
    "@benchmark CUDA.@sync @cuda( \n",
    "    threads = 1024, \n",
    "    blocks = 2,\n",
    "        reduce_atomic(+,   $(CUDA.rand(N, N)), $(CUDA.rand(N, N)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603fb9e8",
   "metadata": {},
   "source": [
    "but we can still improving it using a parallel reduce strategy like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbdd61d",
   "metadata": {},
   "source": [
    "<img src=parallel_reduction.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680193c",
   "metadata": {},
   "source": [
    "The main idea is that each thread reduces its flowing element and so on, finally a the value is written in global mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0616cb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread 1: a[1] + a[2] = 3\n",
      "thread 2: a[3] + a[4] = 7\n",
      "thread 3: a[5] + a[6] = 11\n",
      "thread 4: a[7] + a[8] = 15\n",
      "thread 5: a[9] + a[10] = 19\n",
      "thread 6: a[11] + a[12] = 23\n",
      "thread 7: a[13] + a[14] = 27\n",
      "thread 8: a[15] + a[16] = 31\n",
      "stride = 2\n",
      "thread 1: a[1] + a[3] = 10\n",
      "thread 2: a[5] + a[7] = 26\n",
      "thread 3: a[9] + a[11] = 42\n",
      "thread 4: a[13] + a[15] = 58\n",
      "stride = 4\n",
      "thread 1: a[1] + a[5] = 36\n",
      "thread 2: a[9] + a[13] = 100\n",
      "stride = 8\n",
      "thread 1: a[1] + a[9] = 136\n",
      "stride = 16\n",
      "stride = 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: \u001b[90m#= In[9]:41 =#\u001b[39m CUDA.@allowscalar d_b[] == sum(c_a)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets start with a single block reduction \n",
    "\n",
    "function reduce_block(op, a, b)\n",
    "    num_elements = blockDim().x*2\n",
    "    thread = threadIdx().x\n",
    "    \n",
    "    #parallel reduction of values in a block (stride or distance between each thread reduction) \n",
    "    stride = 1\n",
    "    # while still have elements to reduce \n",
    "    while stride < num_elements\n",
    "        # add a barrier to sync threads\n",
    "        sync_threads()\n",
    "        # compute index to reduce \n",
    "        index = 2*stride*(thread - 1) + 1 \n",
    "        # check index and index + d are inbounds a\n",
    "        @inbounds if index ≤ num_elements && index + stride ≤ length(a)\n",
    "            CUDA.@cuprintln (\"thread $thread: a[$index] + a[$(index + stride)] = $(a[index] + a[index + stride])\")\n",
    "            a[index] = op(a[index], a[index + stride])\n",
    "        end\n",
    "        stride *= 2\n",
    "        thread == 1 && CUDA.@cuprintln(\"stride = $stride\")\n",
    "    end\n",
    "    # download the block reduction to global mem\n",
    "    if thread == 1 \n",
    "        b[] = a[1]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "c_a = 1:16\n",
    "d_a = CuArray(1:16)\n",
    "d_b = CuArray([0])\n",
    "# lunch kernel\n",
    "@cuda(\n",
    "    threads = 16,\n",
    "    blocks = 1,\n",
    "    reduce_block(+, d_a, d_b)\n",
    "    )\n",
    "# test the result \n",
    "using Test\n",
    "@test CUDA.@allowscalar d_b[] == sum(c_a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552911d6",
   "metadata": {},
   "source": [
    "We want that different threads access to consecutive memory items (coalesced access pattern). Now we can extend the implementation for a general grid with multiple blocks. Now, we do the reduction for each block and then do an atomic operation to a global mem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ec8c4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: \u001b[90m#= In[10]:45 =#\u001b[39m CUDA.@allowscalar d_b[] == sum(c_a)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function reduce_grid_atomic(op, a, b)\n",
    "    num_elements = blockDim().x*2\n",
    "    thread = threadIdx().x\n",
    "    block = blockIdx().x\n",
    "    \n",
    "    #parallel reduction of values in a block (stride or distance between each thread reduction) \n",
    "    stride_threads = 1\n",
    "    # parallel reduction between blocks has a stride of \n",
    "    stride_blocks = (block - 1)*num_elements\n",
    "\n",
    "    \n",
    "    # while still have elements to reduce \n",
    "    while stride_threads < num_elements\n",
    "        # add a barrier to sync threads\n",
    "        sync_threads()\n",
    "        # compute index to reduce \n",
    "        index = 2*stride_threads*(thread - 1) + 1 \n",
    "        # check index and index + d are inbounds a\n",
    "        @inbounds if index ≤ num_elements && index + stride_threads + stride_blocks ≤ length(a)\n",
    "#             CUDA.@cuprintln (\"thread $thread: a[$index] + a[$(index + stride_blocks)] = $(a[index] + a[index + stride_blocks])\")\n",
    "            a[stride_blocks + index] = op(a[index + stride_blocks], a[index + stride_threads + stride_blocks])\n",
    "        end\n",
    "        stride_threads *= 2\n",
    "    end\n",
    "    # do attomic operatios with the first entry of ech block (sum through each block)\n",
    "    if thread == 1 \n",
    "        CUDA.@atomic b[] = op(b[], a[stride_blocks + 1])\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "# define test inputs\n",
    "c_a = 1:16\n",
    "d_a = CuArray(1:16)\n",
    "d_b = CuArray([0])\n",
    "# lunch kernel\n",
    "@cuda(\n",
    "    threads = 2,\n",
    "    blocks = 4,\n",
    "    reduce_grid_atomic(+, d_a, d_b)\n",
    "    )\n",
    "# test the result \n",
    "using Test\n",
    "CUDA.@allowscalar d_b\n",
    "@test CUDA.@allowscalar d_b[] == sum(c_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0870ab3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 4 evaluations.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m7.010 μs\u001b[22m\u001b[39m … \u001b[35m119.531 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m7.774 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m8.513 μs\u001b[22m\u001b[39m ± \u001b[32m  3.920 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m▇\u001b[39m█\u001b[39m█\u001b[34m▇\u001b[39m\u001b[39m▆\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[32m▄\u001b[39m\u001b[39m▃\u001b[39m▂\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m \u001b[39m▂\n",
       "  \u001b[39m▄\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▆\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▆\u001b[39m▇\u001b[39m▇\u001b[39m▆\u001b[39m▇\u001b[39m▆\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▃\u001b[39m▃\u001b[39m▅\u001b[39m▅\u001b[39m▃\u001b[39m▂\u001b[39m▃\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m \u001b[39m█\n",
       "  7.01 μs\u001b[90m      \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m      16.3 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m304 bytes\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m5\u001b[39m."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 2048\n",
    "@benchmark CUDA.@sync @cuda( \n",
    "    threads = 1024, \n",
    "    blocks = 2,\n",
    "        reduce_grid_atomic(+,   $(CUDA.rand(N, N)), $(CUDA.rand(N, N)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc71c400",
   "metadata": {},
   "source": [
    "Another option is to implement the reduce technique using shared memory that each multiprocessor has, the reduce is done into a shared memory variable and finally an atomic operation is used to download the data to global memory. The shared memory is used as a cache memory and also to communicate between threads. We can use shared memory as a buffer (we should avoid here bank conflicts (at the same operation threads access to the same bank slot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef6f0b4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: \u001b[90m#= In[12]:46 =#\u001b[39m CUDA.@allowscalar d_b[] == sum(c_a)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function reduce_grid_shared(op, a::AbstractArray{T}, b) where {T}\n",
    "    num_elements = blockDim().x*2\n",
    "    thread = threadIdx().x\n",
    "    block = blockIdx().x\n",
    "    #parallel reduction of values in a block (stride or distance between each thread reduction) \n",
    "    stride_threads = 1\n",
    "    # parallel reduction between blocks has a stride of \n",
    "    stride_blocks = (block - 1)*num_elements\n",
    "    \n",
    "    # shared mem to buffer the a elements\n",
    "    shared = @cuStaticSharedMem(T, (1024,))\n",
    "    @inbounds shared[thread] = a[thread + stride_blocks]\n",
    "    @inbounds shared[thread + blockDim().x] = a[thread + stride_blocks + blockDim().x]\n",
    " \n",
    "    # while still have elements to reduce \n",
    "    while stride_threads < num_elements\n",
    "        # add a barrier to sync threads\n",
    "        sync_threads()\n",
    "        # compute index to reduce \n",
    "        index = 2*stride_threads*(thread - 1) + 1 \n",
    "        # check index and index + d are inbounds a\n",
    "        @inbounds if index ≤ num_elements && index + stride_threads + stride_blocks ≤ length(a)\n",
    "            shared[index] = op(shared[index], shared[index + stride_threads])\n",
    "        end\n",
    "        stride_threads *= 2\n",
    "    end\n",
    "    # do attomic operatios with the first entry of ech block reduction at shared \n",
    "    if thread == 1 \n",
    "        CUDA.@atomic b[] = op(b[], shared[1])\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "# define test inputs\n",
    "c_a = 1:16\n",
    "d_a = CuArray(1:16)\n",
    "d_b = CuArray([0])\n",
    "# lunch kernel\n",
    "@cuda(\n",
    "    threads = 4,\n",
    "    blocks = 2,\n",
    "    reduce_grid_shared(+, d_a, d_b)\n",
    "    )\n",
    "# test the result \n",
    "using Test\n",
    "CUDA.@allowscalar d_b\n",
    "@test CUDA.@allowscalar d_b[] == sum(c_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "452781ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 4 evaluations.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m7.612 μs\u001b[22m\u001b[39m … \u001b[35m123.064 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m8.185 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m8.666 μs\u001b[22m\u001b[39m ± \u001b[32m  3.403 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m▅\u001b[39m▇\u001b[39m█\u001b[34m▆\u001b[39m\u001b[39m▅\u001b[32m▃\u001b[39m\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m▂\n",
       "  \u001b[39m▆\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▅\u001b[39m▆\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m█\u001b[39m▆\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▁\u001b[39m▄\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▄\u001b[39m▁\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▇\u001b[39m█\u001b[39m▆\u001b[39m▃\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m█\u001b[39m \u001b[39m█\n",
       "  7.61 μs\u001b[90m      \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m      17.2 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m304 bytes\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m5\u001b[39m."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test it \n",
    "N = 2048\n",
    "@benchmark CUDA.@sync @cuda( \n",
    "    threads = 1024, \n",
    "    blocks = 2,\n",
    "        reduce_grid_shared(+,   $(CUDA.rand(N, N)), $(CUDA.rand(N, N)))\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
