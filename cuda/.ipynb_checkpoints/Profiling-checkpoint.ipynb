{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56085416",
   "metadata": {},
   "source": [
    "# Profiling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01510308",
   "metadata": {},
   "source": [
    "Often is really important to obtain a profiling for our GPU program, to check coalsceed access, race conditions problems, memory managment, etc. For such task, we can call `nvprof` tool from NVIDIA. On a Unix system we should execute:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9edae8",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ nvprof --profile-from-start off /path/to/julia\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dba162",
   "metadata": {},
   "source": [
    "The `/path/to/julia` is the path to julia binary. Note that we don't initialize immediately the profiler but we can call the CUDA API's with the macro @profile:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fdb19a",
   "metadata": {},
   "source": [
    "```julia\n",
    "CUDA.@profile kernel_name(x_d, y_d, r_d)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192595e2",
   "metadata": {},
   "source": [
    "But nvprof is not longer used for GPUs with compute capalities newer than 7.0, instead we need nsys (Nsight system), to set nsys to julia we run: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56123830",
   "metadata": {},
   "source": [
    "```bash\n",
    "nsys launch julia --trace=cuda \n",
    "```\n",
    "then we can execute the previous code but adding the macro CUDA.@profile before we lunch the kernel:\n",
    "```julia \n",
    "julia> using CUDA\n",
    "\n",
    "julia> a = CUDA.rand(1024,1024,1024);\n",
    "\n",
    "julia> sin.(a);\n",
    "\n",
    "julia> CUDA.@profile sin.(a);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046c8a62",
   "metadata": {},
   "source": [
    "Then we open Nshight System using `nsys-cli` and open the report. If we want the old nvprof output we can execute "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cef7e0",
   "metadata": {},
   "source": [
    "```bash\n",
    "nsys nvprof --profile-from-start off /path/to/julia --trace=cuda\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187dadc1",
   "metadata": {},
   "source": [
    "then we can execute the previous code or the kernel we want to profile adding the macro CUDA.@profile before we lunch the kernel:\n",
    "```julia \n",
    "julia> using CUDA\n",
    "\n",
    "julia> a = CUDA.rand(1024,1024,1024);\n",
    "\n",
    "julia> sin.(a);\n",
    "\n",
    "julia> CUDA.@profile sin.(a);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710a982e",
   "metadata": {},
   "source": [
    "## Profiling a reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5399d4",
   "metadata": {},
   "source": [
    "We paste here our both atomic and shared mem versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28558455",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reduce_grid_shared (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools, CUDA, Test\n",
    "\n",
    "#######################\n",
    "# REDUCE GRID ATOMIC\n",
    "#######################\n",
    "function reduce_grid_atomic(op, a, b)\n",
    "    num_elements = blockDim().x*2\n",
    "    thread = threadIdx().x\n",
    "    block = blockIdx().x\n",
    "    \n",
    "    #parallel reduction of values in a block (stride or distance between each thread reduction) \n",
    "    stride_threads = 1\n",
    "    # parallel reduction between blocks has a stride of \n",
    "    stride_blocks = (block - 1)*num_elements\n",
    "\n",
    "    \n",
    "    # while still have elements to reduce \n",
    "    while stride_threads < num_elements\n",
    "        # add a barrier to sync threads\n",
    "        sync_threads()\n",
    "        # compute index to reduce \n",
    "        index = 2*stride_threads*(thread - 1) + 1 \n",
    "        # check index and index + d are inbounds a\n",
    "        @inbounds if index ≤ num_elements && index + stride_threads + stride_blocks ≤ length(a)\n",
    "#             CUDA.@cuprintln (\"thread $thread: a[$index] + a[$(index + stride_blocks)] = $(a[index] + a[index + stride_blocks])\")\n",
    "            a[stride_blocks + index] = op(a[index + stride_blocks], a[index + stride_threads + stride_blocks])\n",
    "        end\n",
    "        stride_threads *= 2\n",
    "    end\n",
    "    # do attomic operatios with the first entry of ech block (sum through each block)\n",
    "    if thread == 1 \n",
    "        CUDA.@atomic b[] = op(b[], a[stride_blocks + 1])\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "# define test inputs\n",
    "c_a = 1:16\n",
    "d_a = CuArray(1:16)\n",
    "d_b = CuArray([0])\n",
    "# lunch kernel\n",
    "@cuda(\n",
    "    threads = 2,\n",
    "    blocks = 4,\n",
    "    reduce_grid_atomic(+, d_a, d_b)\n",
    "    )\n",
    "# test the result \n",
    "using Test\n",
    "CUDA.@allowscalar d_b\n",
    "@test CUDA.@allowscalar d_b[] == sum(c_a)\n",
    "\n",
    "\n",
    "#######################\n",
    "# REDUCE SHARED MEMORY\n",
    "#######################\n",
    "function reduce_grid_shared(op, a::AbstractArray{T}, b) where {T}\n",
    "    num_elements = blockDim().x*2\n",
    "    thread = threadIdx().x\n",
    "    block = blockIdx().x\n",
    "    #parallel reduction of values in a block (stride or distance between each thread reduction) \n",
    "    stride_threads = 1\n",
    "    # parallel reduction between blocks has a stride of \n",
    "    stride_blocks = (block - 1)*num_elements\n",
    "    \n",
    "    # shared mem to buffer the a elements\n",
    "    shared = @cuStaticSharedMem(T, (2048,))\n",
    "    @inbounds shared[thread] = a[thread + stride_blocks]\n",
    "    @inbounds shared[thread + blockDim().x] = a[thread + stride_blocks + blockDim().x]\n",
    " \n",
    "    # while still have elements to reduce \n",
    "    while stride_threads < num_elements\n",
    "        # add a barrier to sync threads\n",
    "        sync_threads()\n",
    "        # compute index to reduce \n",
    "        index = 2*stride_threads*(thread - 1) + 1 \n",
    "        # check index and index + d are inbounds a\n",
    "        @inbounds if index ≤ num_elements && index + stride_threads + stride_blocks ≤ length(a)\n",
    "            shared[index] = op(shared[index], shared[index + stride_threads])\n",
    "        end\n",
    "        stride_threads *= 2\n",
    "    end\n",
    "    # do attomic operatios with the first entry of ech block reduction at shared \n",
    "    if thread == 1 \n",
    "        CUDA.@atomic b[] = op(b[], shared[1])\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fe12d3",
   "metadata": {},
   "source": [
    "### Testing both reduce implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552e06c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: \u001b[90m#= In[3]:25 =#\u001b[39m CUDA.@allowscalar d_b[] == sum(c_a)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define test inputs\n",
    "c_a = 1:16\n",
    "d_a = CuArray(1:16)\n",
    "d_b = CuArray([0])\n",
    "# lunch kernel shared\n",
    "@cuda(\n",
    "    threads = 4,\n",
    "    blocks = 2,\n",
    "    reduce_grid_atomic(+, d_a, d_b)\n",
    "    )\n",
    "# test the result \n",
    "@test CUDA.@allowscalar d_b[] == sum(c_a)\n",
    "# re-define test inputs\n",
    "CUDA.unsafe_free!(d_b)\n",
    "CUDA.unsafe_free!(d_a)\n",
    "d_a = CuArray(1:16)\n",
    "d_b = CuArray([0])\n",
    "\n",
    "# lunch kernel shared\n",
    "@cuda(\n",
    "    threads = 4,\n",
    "    blocks = 2,\n",
    "    reduce_grid_shared(+, d_a, d_b)\n",
    "    )\n",
    "@test CUDA.@allowscalar d_b[] == sum(c_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537aae63",
   "metadata": {},
   "source": [
    "Then we just define two different functions first `main()` function to profile our kernels and then `my_reduce()` to call both reducntion kernels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fccaa62f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_reduce (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function my_reduce(op::Function, a::AbstractArray{T}) where {T}\n",
    "    # launch atomic reduction\n",
    "    a_atomic = copy(a) \n",
    "    b_atomic = CUDA.zeros(T, 1)\n",
    "\n",
    "    kernel_atomic = @cuda(\n",
    "        launch=false,\n",
    "        reduce_grid_atomic(+, a_atomic, b_atomic)\n",
    "    ) \n",
    "\n",
    "    config = launch_configuration(kernel_atomic.fun)\n",
    "    threads_config = min(config.threads, length(a))\n",
    "    threads = 1024\n",
    "    blocks = cld(length(a_atomic), threads*2)\n",
    "\n",
    "    @cuda(\n",
    "        threads=threads,\n",
    "        blocks=blocks,\n",
    "        reduce_grid_atomic(op, a_atomic, b_atomic)\n",
    "    ) \n",
    "    # launch shared memory  reduction\n",
    "    b_shared = CUDA.zeros(T, 1)\n",
    "\n",
    "    kernel_shared = @cuda(\n",
    "        launch=false,\n",
    "        reduce_grid_shared(+, a, b_shared)\n",
    "    ) \n",
    "\n",
    "    config = launch_configuration(kernel_shared.fun)\n",
    "    threads_config = min(config.threads, length(a))\n",
    "    threads = 1024\n",
    "    blocks = cld(length(a), threads*2)\n",
    "\n",
    "    @cuda(\n",
    "        threads=threads,\n",
    "        blocks=blocks,\n",
    "        reduce_grid_atomic(op, a, b_shared)\n",
    "    ) \n",
    "    # test outputs\n",
    "    @assert b_shared ≈ b_atomic\n",
    "\n",
    "    CUDA.@allowscalar b_atomic[]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd4686c",
   "metadata": {},
   "source": [
    "We just added the `@profile` macro and the `NVTX.@range`which makes it possible to enrich the profile tracer. Also to execute the function twice because sometimes an overhead is incurred at the first call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79553bf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4076861675728695e6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "main()\n",
    "\n",
    "function to launch the kernel\n",
    "\"\"\"\n",
    "function main()\n",
    "    N = 1024\n",
    "    c_a = rand(N,N)\n",
    "    d_a = CuArray(c_a)\n",
    "    @test my_reduce(+, d_a) ≈ sum(c_a)\n",
    "\n",
    "    # profile it \n",
    "    CUDA.@profile begin \n",
    "        NVTX.@range \"my_reduce\" my_reduce(+, d_a)\n",
    "        NVTX.@range \"my_reduce\" my_reduce(+, d_a)\n",
    "    end\n",
    "# execute the function\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c8f004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
