{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8520f713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=Philosopher:400,700,400italic,700italic' rel='stylesheet' type='text/css'>\r\n",
       "\r\n",
       "<style>\r\n",
       "\r\n",
       "@font-face {\r\n",
       "    font-family: \"Computer Modern\";\r\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "\r\n",
       "/* Formatting for header cells */\r\n",
       ".text_cell_render h1 {\r\n",
       "    font-family: 'Philosopher', sans-serif;\r\n",
       "    font-weight: 400;\r\n",
       "    font-size: 2.2em;\r\n",
       "    line-height: 100%;\r\n",
       "    color: rgb(0, 80, 120);\r\n",
       "    margin-bottom: 0.1em;\r\n",
       "    margin-top: 0.1em;\r\n",
       "    display: block;\r\n",
       "}\t\r\n",
       ".text_cell_render h2 {\r\n",
       "    font-family: 'Philosopher', serif;\r\n",
       "    font-weight: 400;\r\n",
       "    font-size: 1.9em;\r\n",
       "    line-height: 100%;\r\n",
       "    color: rgb(200,100,0);\r\n",
       "    margin-bottom: 0.1em;\r\n",
       "    margin-top: 0.1em;\r\n",
       "    display: block;\r\n",
       "}\t\r\n",
       "\r\n",
       ".text_cell_render h3 {\r\n",
       "    font-family: 'Philosopher', serif;\r\n",
       "    margin-top:12px;\r\n",
       "    margin-bottom: 3px;\r\n",
       "    font-style: italic;\r\n",
       "    color: rgb(94,127,192);\r\n",
       "}\r\n",
       "\r\n",
       ".text_cell_render h4 {\r\n",
       "    font-family: 'Philosopher', serif;\r\n",
       "}\r\n",
       "\r\n",
       ".text_cell_render h5 {\r\n",
       "    font-family: 'Alegreya Sans', sans-serif;\r\n",
       "    font-weight: 300;\r\n",
       "    font-size: 16pt;\r\n",
       "    color: grey;\r\n",
       "    font-style: italic;\r\n",
       "    margin-bottom: .1em;\r\n",
       "    margin-top: 0.1em;\r\n",
       "    display: block;\r\n",
       "}\r\n",
       "\r\n",
       ".text_cell_render h6 {\r\n",
       "    font-family: 'PT Mono', sans-serif;\r\n",
       "    font-weight: 300;\r\n",
       "    font-size: 10pt;\r\n",
       "    color: grey;\r\n",
       "    margin-bottom: 1px;\r\n",
       "    margin-top: 1px;\r\n",
       "}\r\n",
       "\r\n",
       ".CodeMirror{\r\n",
       "        font-family: \"PT Mono\";\r\n",
       "        font-size: 100%;\r\n",
       "}\r\n",
       "\r\n",
       "</style>\r\n",
       "\r\n"
      ],
      "text/plain": [
       "HTML{String}(\"<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\\r\\n<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\\r\\n<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\\r\\n<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\\r\\n<link href='http://fonts.googleapis.com/css?family=Philosopher:400,700,400italic,700italic' rel='stylesheet' type='text/css'>\\r\\n\\r\\n<style>\\r\\n\\r\\n@font-face {\\r\\n    font-family: \\\"Computer Modern\\\";\\r\\n    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\\r\\n}\\r\\n\\r\\n\\r\\n\\r\\n/* Formatting for header cells */\\r\\n.text_cell_render h1 {\\r\\n    font-family: 'Philosopher', sans-serif;\\r\\n    font-weight: 400;\\r\\n    font-size: 2.2em;\\r\\n    line-height: 100%;\\r\\n    color: rgb(0, 80, 120);\\r\\n    margin-bottom: 0.1em;\\r\\n    margin-top: 0.1em;\\r\\n    display: block;\\r\\n}\\t\\r\\n.text_cell_render h2 {\\r\\n    font-family: 'Philosopher', serif;\\r\\n    font-weight: 400;\\r\\n    font-size: 1.9em;\\r\\n    line-height: 100%;\\r\\n    color: rgb(200,100,0);\\r\\n    margin-bottom: 0.1em;\\r\\n    margin-top: 0.1em;\\r\\n    display: block;\\r\\n}\\t\\r\\n\\r\\n.text_cell_render h3 {\\r\\n    font-family: 'Philosopher', serif;\\r\\n    margin-top:12px;\\r\\n    margin-bottom: 3px;\\r\\n    font-style: italic;\\r\\n    color: rgb(94,127,192);\\r\\n}\\r\\n\\r\\n.text_cell_render h4 {\\r\\n    font-family: 'Philosopher', serif;\\r\\n}\\r\\n\\r\\n.text_cell_render h5 {\\r\\n    font-family: 'Alegreya Sans', sans-serif;\\r\\n    font-weight: 300;\\r\\n    font-size: 16pt;\\r\\n    color: grey;\\r\\n    font-style: italic;\\r\\n    margin-bottom: .1em;\\r\\n    margin-top: 0.1em;\\r\\n    display: block;\\r\\n}\\r\\n\\r\\n.text_cell_render h6 {\\r\\n    font-family: 'PT Mono', sans-serif;\\r\\n    font-weight: 300;\\r\\n    font-size: 10pt;\\r\\n    color: grey;\\r\\n    margin-bottom: 1px;\\r\\n    margin-top: 1px;\\r\\n}\\r\\n\\r\\n.CodeMirror{\\r\\n        font-family: \\\"PT Mono\\\";\\r\\n        font-size: 100%;\\r\\n}\\r\\n\\r\\n</style>\\r\\n\\r\\n\")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Setting up a custom stylesheet in IJulia\n",
    "file = open(\"./../style.css\") # A .css file in the same folder as this notebook file\n",
    "styl = read(file, String) # Read the file\n",
    "HTML(\"$styl\") # Output as HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d418c77",
   "metadata": {},
   "source": [
    "## General limitations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd7d7e0",
   "metadata": {},
   "source": [
    "- poor Float64 performance: GPU\n",
    "- no recursion : CUDA.jl, though CUDA’s support is limited by the GPU\n",
    "- kernel must return nothing : CUDA\n",
    "- no kernel varargs: CUDA, and so far I have not seen CUDA.jl kernels with VarArgs\n",
    "- no strings : CUDA (aside: a workaround is using arrays of Char, but I haven’t seen CUDA.jl examples of those so far)\n",
    "- must have type-inferred code : CUDA C is statically typed\n",
    "- no garbage collection on device : CUDA C has manual memory management\n",
    "- kernel cannot allocate, and only isbits types in device arrays: CUDA C has no garbage collection, and Julia has no manual deallocations, let alone on the - - device to deal with data that live independently of the CuArray.\n",
    "- no try-catch-finally in kernel: CUDA C does not support exception handling on device (v11.5.1 docs, Programming Guide I.4.7)\n",
    "- no scalar indexing of CuArray: just a scalar getindex in host code must put scalar on CPU, and CUDA.jl does not launch kernel for scalar setindex e.g. a[1] - - += 1 because it’s not worth it\n",
    "- calls to CPU-only runtime library: GPU can’t have a version of every low-level CPU function Julia has"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa1f7ab",
   "metadata": {},
   "source": [
    "# Unupported kernel operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e872fa",
   "metadata": {},
   "source": [
    "The most frequent issues with the GPU stack come from expected CPU functionality not being implemented or supported on the GPU. We have different array operations that leads to an error:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075a645d",
   "metadata": {},
   "source": [
    "## Unsupported array operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d13640",
   "metadata": {},
   "source": [
    "For example with array expressions operations often lead to iterating a fallback functionality, which triggers the following scalar iteration error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ef076f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Performing scalar indexing on task Task (runnable) @0x00007fed000619f0.\n",
      "│ Invocation of getindex resulted in scalar indexing of a GPU array.\n",
      "│ This is typically caused by calling an iterating implementation of a method.\n",
      "│ Such implementations *do not* execute on the GPU, but very slowly on the CPU,\n",
      "│ and therefore are only permitted from the REPL for prototyping purposes.\n",
      "│ If you did intend to index this array, annotate the caller with @allowscalar.\n",
      "└ @ GPUArraysCore /home/mvanzulli/.julia/packages/GPUArraysCore/rSIl2/src/GPUArraysCore.jl:81\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: cannot take the CPU address of a CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}",
     "output_type": "error",
     "traceback": [
      "ArgumentError: cannot take the CPU address of a CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}",
      "",
      "Stacktrace:",
      " [1] unsafe_convert(#unused#::Type{Ptr{Float32}}, x::CuArray{Float32, 2, CUDA.Mem.DeviceBuffer})",
      "   @ CUDA ~/.julia/packages/CUDA/tTK8Y/src/array.jl:319",
      " [2] geevx!(balanc::Char, jobvl::Char, jobvr::Char, sense::Char, A::CuArray{Float32, 2, CUDA.Mem.DeviceBuffer})",
      "   @ LinearAlgebra.LAPACK /usr/share/julia/stdlib/v1.7/LinearAlgebra/src/lapack.jl:2100",
      " [3] eigen!(A::CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}; permute::Bool, scale::Bool, sortby::typeof(LinearAlgebra.eigsortby))",
      "   @ LinearAlgebra /usr/share/julia/stdlib/v1.7/LinearAlgebra/src/eigen.jl:152",
      " [4] eigen(A::CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}; permute::Bool, scale::Bool, sortby::typeof(LinearAlgebra.eigsortby))",
      "   @ LinearAlgebra /usr/share/julia/stdlib/v1.7/LinearAlgebra/src/eigen.jl:237",
      " [5] eigen(A::CuArray{Float32, 2, CUDA.Mem.DeviceBuffer})",
      "   @ LinearAlgebra /usr/share/julia/stdlib/v1.7/LinearAlgebra/src/eigen.jl:235",
      " [6] top-level scope",
      "   @ In[2]:3",
      " [7] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [8] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "using CUDA, LinearAlgebra\n",
    "\n",
    "eigen(CUDA.rand(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91347f17",
   "metadata": {},
   "source": [
    "In this case the generic.jl function uses for loops which produces an undesire overhead. Hence are not allowed, in this case we should use CUBLAS  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3cfe6",
   "metadata": {},
   "source": [
    "With array operations, it's also easy to get an error when using unsupported data with GPU kernels. Essentially, every value passed to a kernel needs to be an isbits type. An easy way to violate this, is to pass a CPU array to a GPU array operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cff1edf9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "GPU compilation of kernel #broadcast_kernel#17(CUDA.CuKernelContext, CuDeviceMatrix{Float64, 1}, Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2}, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}, var\"#1#2\", Tuple{Base.Broadcast.Extruded{CuDeviceMatrix{Float32, 1}, Tuple{Bool, Bool}, Tuple{Int64, Int64}}, Base.Broadcast.Extruded{Vector{Float64}, Tuple{Bool}, Tuple{Int64}}}}, Int64) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 4 to your kernel function is of type Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2}, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}, var\"#1#2\", Tuple{Base.Broadcast.Extruded{CuDeviceMatrix{Float32, 1}, Tuple{Bool, Bool}, Tuple{Int64, Int64}}, Base.Broadcast.Extruded{Vector{Float64}, Tuple{Bool}, Tuple{Int64}}}}, which is not isbits:\n  .args is of type Tuple{Base.Broadcast.Extruded{CuDeviceMatrix{Float32, 1}, Tuple{Bool, Bool}, Tuple{Int64, Int64}}, Base.Broadcast.Extruded{Vector{Float64}, Tuple{Bool}, Tuple{Int64}}} which is not isbits.\n    .2 is of type Base.Broadcast.Extruded{Vector{Float64}, Tuple{Bool}, Tuple{Int64}} which is not isbits.\n      .x is of type Vector{Float64} which is not isbits.\n\n",
     "output_type": "error",
     "traceback": [
      "GPU compilation of kernel #broadcast_kernel#17(CUDA.CuKernelContext, CuDeviceMatrix{Float64, 1}, Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2}, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}, var\"#1#2\", Tuple{Base.Broadcast.Extruded{CuDeviceMatrix{Float32, 1}, Tuple{Bool, Bool}, Tuple{Int64, Int64}}, Base.Broadcast.Extruded{Vector{Float64}, Tuple{Bool}, Tuple{Int64}}}}, Int64) failed\nKernelError: passing and using non-bitstype argument\n\nArgument 4 to your kernel function is of type Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2}, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}, var\"#1#2\", Tuple{Base.Broadcast.Extruded{CuDeviceMatrix{Float32, 1}, Tuple{Bool, Bool}, Tuple{Int64, Int64}}, Base.Broadcast.Extruded{Vector{Float64}, Tuple{Bool}, Tuple{Int64}}}}, which is not isbits:\n  .args is of type Tuple{Base.Broadcast.Extruded{CuDeviceMatrix{Float32, 1}, Tuple{Bool, Bool}, Tuple{Int64, Int64}}, Base.Broadcast.Extruded{Vector{Float64}, Tuple{Bool}, Tuple{Int64}}} which is not isbits.\n    .2 is of type Base.Broadcast.Extruded{Vector{Float64}, Tuple{Bool}, Tuple{Int64}} which is not isbits.\n      .x is of type Vector{Float64} which is not isbits.\n\n",
      "",
      "Stacktrace:",
      "  [1] check_invocation(job::GPUCompiler.CompilerJob)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/iaKrd/src/validation.jl:86",
      "  [2] macro expansion",
      "    @ ~/.julia/packages/GPUCompiler/iaKrd/src/driver.jl:413 [inlined]",
      "  [3] macro expansion",
      "    @ ~/.julia/packages/TimerOutputs/jgSVI/src/TimerOutput.jl:252 [inlined]",
      "  [4] macro expansion",
      "    @ ~/.julia/packages/GPUCompiler/iaKrd/src/driver.jl:412 [inlined]",
      "  [5] emit_asm(job::GPUCompiler.CompilerJob, ir::LLVM.Module; strip::Bool, validate::Bool, format::LLVM.API.LLVMCodeGenFileType)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/iaKrd/src/utils.jl:64",
      "  [6] cufunction_compile(job::GPUCompiler.CompilerJob, ctx::LLVM.Context)",
      "    @ CUDA ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:354",
      "  [7] #224",
      "    @ ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:347 [inlined]",
      "  [8] JuliaContext(f::CUDA.var\"#224#225\"{GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams, GPUCompiler.FunctionSpec{GPUArrays.var\"#broadcast_kernel#17\", Tuple{CUDA.CuKernelContext, CuDeviceMatrix{Float64, 1}, Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2}, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}, var\"#1#2\", Tuple{Base.Broadcast.Extruded{CuDeviceMatrix{Float32, 1}, Tuple{Bool, Bool}, Tuple{Int64, Int64}}, Base.Broadcast.Extruded{Vector{Float64}, Tuple{Bool}, Tuple{Int64}}}}, Int64}}}})",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/iaKrd/src/driver.jl:74",
      "  [9] cufunction_compile(job::GPUCompiler.CompilerJob)",
      "    @ CUDA ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:346",
      " [10] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link))",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/iaKrd/src/cache.jl:90",
      " [11] cufunction(f::GPUArrays.var\"#broadcast_kernel#17\", tt::Type{Tuple{CUDA.CuKernelContext, CuDeviceMatrix{Float64, 1}, Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{2}, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}, var\"#1#2\", Tuple{Base.Broadcast.Extruded{CuDeviceMatrix{Float32, 1}, Tuple{Bool, Bool}, Tuple{Int64, Int64}}, Base.Broadcast.Extruded{Vector{Float64}, Tuple{Bool}, Tuple{Int64}}}}, Int64}}; name::Nothing, kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ CUDA ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:299",
      " [12] cufunction",
      "    @ ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:293 [inlined]",
      " [13] macro expansion",
      "    @ ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:102 [inlined]",
      " [14] #launch_heuristic#248",
      "    @ ~/.julia/packages/CUDA/tTK8Y/src/gpuarrays.jl:17 [inlined]",
      " [15] _copyto!",
      "    @ ~/.julia/packages/GPUArrays/gok9K/src/host/broadcast.jl:73 [inlined]",
      " [16] copyto!",
      "    @ ~/.julia/packages/GPUArrays/gok9K/src/host/broadcast.jl:56 [inlined]",
      " [17] copy",
      "    @ ~/.julia/packages/GPUArrays/gok9K/src/host/broadcast.jl:47 [inlined]",
      " [18] materialize",
      "    @ ./broadcast.jl:860 [inlined]",
      " [19] broadcast(::var\"#1#2\", ::CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, ::Vector{Float64})",
      "    @ Base.Broadcast ./broadcast.jl:798",
      " [20] top-level scope",
      "    @ In[3]:3",
      " [21] eval",
      "    @ ./boot.jl:373 [inlined]",
      " [22] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "a = CUDA.rand(2,2)\n",
    "b = rand(2)\n",
    "broadcast(a, b) do x, y\n",
    "    x + y\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f8f18f",
   "metadata": {},
   "source": [
    "## Unsupported kernel operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26566105",
   "metadata": {},
   "source": [
    "In device code, i.e. code that actually runs on the GPU as opposed to a CPU method (like eigen) that's implemented using GPU kernels, the story is a little more complicated. Essentially, not all of the Julia language is supported. If you use unsupported functionality, you will see a compilation error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86c23d77",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "InvalidIRError: compiling kernel #broadcast_kernel#17(CUDA.CuKernelContext, CuDeviceVector{Nothing, 1}, Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{1}, Tuple{Base.OneTo{Int64}}, var\"#3#4\", Tuple{Base.Broadcast.Extruded{CuDeviceVector{Float32, 1}, Tuple{Bool}, Tuple{Int64}}}}, Int64) resulted in invalid LLVM IR\nReason: unsupported call to the Julia runtime (call to jl_subtype)\nStacktrace:\n [1] \u001b[0m\u001b[1mprint\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mcoreio.jl:3\u001b[24m\u001b[39m\n [2] \u001b[0m\u001b[1m#3\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[4]:3\u001b[24m\u001b[39m\n [3] \u001b[0m\u001b[1m_broadcast_getindex_evalf\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:670\u001b[24m\u001b[39m\n [4] \u001b[0m\u001b[1m_broadcast_getindex\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:643\u001b[24m\u001b[39m\n [5] \u001b[0m\u001b[1mgetindex\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:597\u001b[24m\u001b[39m\n [6] \u001b[0m\u001b[1mbroadcast_kernel\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m~/.julia/packages/GPUArrays/gok9K/src/host/\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:67\u001b[24m\u001b[39m\nReason: unsupported dynamic function invocation (call to print)\nStacktrace:\n [1] \u001b[0m\u001b[1mprint\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mcoreio.jl:3\u001b[24m\u001b[39m\n [2] \u001b[0m\u001b[1m#3\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[4]:3\u001b[24m\u001b[39m\n [3] \u001b[0m\u001b[1m_broadcast_getindex_evalf\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:670\u001b[24m\u001b[39m\n [4] \u001b[0m\u001b[1m_broadcast_getindex\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:643\u001b[24m\u001b[39m\n [5] \u001b[0m\u001b[1mgetindex\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:597\u001b[24m\u001b[39m\n [6] \u001b[0m\u001b[1mbroadcast_kernel\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m~/.julia/packages/GPUArrays/gok9K/src/host/\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:67\u001b[24m\u001b[39m\n\u001b[36m\u001b[1mHint\u001b[22m\u001b[39m\u001b[36m: catch this exception as `err` and call `code_typed(err; interactive = true)` to introspect the erronous code\u001b[39m",
     "output_type": "error",
     "traceback": [
      "InvalidIRError: compiling kernel #broadcast_kernel#17(CUDA.CuKernelContext, CuDeviceVector{Nothing, 1}, Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{1}, Tuple{Base.OneTo{Int64}}, var\"#3#4\", Tuple{Base.Broadcast.Extruded{CuDeviceVector{Float32, 1}, Tuple{Bool}, Tuple{Int64}}}}, Int64) resulted in invalid LLVM IR\nReason: unsupported call to the Julia runtime (call to jl_subtype)\nStacktrace:\n [1] \u001b[0m\u001b[1mprint\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mcoreio.jl:3\u001b[24m\u001b[39m\n [2] \u001b[0m\u001b[1m#3\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[4]:3\u001b[24m\u001b[39m\n [3] \u001b[0m\u001b[1m_broadcast_getindex_evalf\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:670\u001b[24m\u001b[39m\n [4] \u001b[0m\u001b[1m_broadcast_getindex\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:643\u001b[24m\u001b[39m\n [5] \u001b[0m\u001b[1mgetindex\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:597\u001b[24m\u001b[39m\n [6] \u001b[0m\u001b[1mbroadcast_kernel\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m~/.julia/packages/GPUArrays/gok9K/src/host/\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:67\u001b[24m\u001b[39m\nReason: unsupported dynamic function invocation (call to print)\nStacktrace:\n [1] \u001b[0m\u001b[1mprint\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mcoreio.jl:3\u001b[24m\u001b[39m\n [2] \u001b[0m\u001b[1m#3\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[4]:3\u001b[24m\u001b[39m\n [3] \u001b[0m\u001b[1m_broadcast_getindex_evalf\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:670\u001b[24m\u001b[39m\n [4] \u001b[0m\u001b[1m_broadcast_getindex\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:643\u001b[24m\u001b[39m\n [5] \u001b[0m\u001b[1mgetindex\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:597\u001b[24m\u001b[39m\n [6] \u001b[0m\u001b[1mbroadcast_kernel\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m~/.julia/packages/GPUArrays/gok9K/src/host/\u001b[39m\u001b[90m\u001b[4mbroadcast.jl:67\u001b[24m\u001b[39m\n\u001b[36m\u001b[1mHint\u001b[22m\u001b[39m\u001b[36m: catch this exception as `err` and call `code_typed(err; interactive = true)` to introspect the erronous code\u001b[39m",
      "",
      "Stacktrace:",
      "  [1] check_ir(job::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams, GPUCompiler.FunctionSpec{GPUArrays.var\"#broadcast_kernel#17\", Tuple{CUDA.CuKernelContext, CuDeviceVector{Nothing, 1}, Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{1}, Tuple{Base.OneTo{Int64}}, var\"#3#4\", Tuple{Base.Broadcast.Extruded{CuDeviceVector{Float32, 1}, Tuple{Bool}, Tuple{Int64}}}}, Int64}}}, args::LLVM.Module)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/iaKrd/src/validation.jl:139",
      "  [2] macro expansion",
      "    @ ~/.julia/packages/GPUCompiler/iaKrd/src/driver.jl:414 [inlined]",
      "  [3] macro expansion",
      "    @ ~/.julia/packages/TimerOutputs/jgSVI/src/TimerOutput.jl:252 [inlined]",
      "  [4] macro expansion",
      "    @ ~/.julia/packages/GPUCompiler/iaKrd/src/driver.jl:412 [inlined]",
      "  [5] emit_asm(job::GPUCompiler.CompilerJob, ir::LLVM.Module; strip::Bool, validate::Bool, format::LLVM.API.LLVMCodeGenFileType)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/iaKrd/src/utils.jl:64",
      "  [6] cufunction_compile(job::GPUCompiler.CompilerJob, ctx::LLVM.Context)",
      "    @ CUDA ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:354",
      "  [7] #224",
      "    @ ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:347 [inlined]",
      "  [8] JuliaContext(f::CUDA.var\"#224#225\"{GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams, GPUCompiler.FunctionSpec{GPUArrays.var\"#broadcast_kernel#17\", Tuple{CUDA.CuKernelContext, CuDeviceVector{Nothing, 1}, Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{1}, Tuple{Base.OneTo{Int64}}, var\"#3#4\", Tuple{Base.Broadcast.Extruded{CuDeviceVector{Float32, 1}, Tuple{Bool}, Tuple{Int64}}}}, Int64}}}})",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/iaKrd/src/driver.jl:74",
      "  [9] cufunction_compile(job::GPUCompiler.CompilerJob)",
      "    @ CUDA ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:346",
      " [10] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link))",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/iaKrd/src/cache.jl:90",
      " [11] cufunction(f::GPUArrays.var\"#broadcast_kernel#17\", tt::Type{Tuple{CUDA.CuKernelContext, CuDeviceVector{Nothing, 1}, Base.Broadcast.Broadcasted{CUDA.CuArrayStyle{1}, Tuple{Base.OneTo{Int64}}, var\"#3#4\", Tuple{Base.Broadcast.Extruded{CuDeviceVector{Float32, 1}, Tuple{Bool}, Tuple{Int64}}}}, Int64}}; name::Nothing, kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ CUDA ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:299",
      " [12] cufunction",
      "    @ ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:293 [inlined]",
      " [13] macro expansion",
      "    @ ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:102 [inlined]",
      " [14] #launch_heuristic#248",
      "    @ ~/.julia/packages/CUDA/tTK8Y/src/gpuarrays.jl:17 [inlined]",
      " [15] _copyto!",
      "    @ ~/.julia/packages/GPUArrays/gok9K/src/host/broadcast.jl:73 [inlined]",
      " [16] copyto!",
      "    @ ~/.julia/packages/GPUArrays/gok9K/src/host/broadcast.jl:56 [inlined]",
      " [17] copy",
      "    @ ~/.julia/packages/GPUArrays/gok9K/src/host/broadcast.jl:47 [inlined]",
      " [18] materialize",
      "    @ ./broadcast.jl:860 [inlined]",
      " [19] broadcast(f::var\"#3#4\", As::CuArray{Float32, 1, CUDA.Mem.DeviceBuffer})",
      "    @ Base.Broadcast ./broadcast.jl:798",
      " [20] top-level scope",
      "    @ In[4]:2",
      " [21] eval",
      "    @ ./boot.jl:373 [inlined]",
      " [22] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "a = CUDA.rand(1)\n",
    "broadcast(a) do x\n",
    "    print(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5546e1ce",
   "metadata": {},
   "source": [
    "To help debugging this, there's multiple stacktraces being displayed: one pointing to each unsupported operation in a GPU kernel, and finally a host stack trace pointing to the CPU code that invoked the kernel. Often, that's sufficient to find and resolve the issue.\n",
    "\n",
    "Sometimes though, the issue is more subtle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7ee3bba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "InvalidIRError: compiling kernel #bad_kernel(CuDeviceVector{Int64, 1}) resulted in invalid LLVM IR\nReason: unsupported use of an undefined name (use of 'threadId')\nStacktrace:\n [1] \u001b[0m\u001b[1mbad_kernel\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[5]:2\u001b[24m\u001b[39m\nReason: unsupported dynamic function invocation\nStacktrace:\n [1] \u001b[0m\u001b[1mbad_kernel\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[5]:2\u001b[24m\u001b[39m\nReason: unsupported dynamic function invocation (call to getproperty)\nStacktrace:\n [1] \u001b[0m\u001b[1mbad_kernel\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[5]:2\u001b[24m\u001b[39m\nReason: unsupported dynamic function invocation (call to setindex!)\nStacktrace:\n [1] \u001b[0m\u001b[1mbad_kernel\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[5]:2\u001b[24m\u001b[39m\n\u001b[36m\u001b[1mHint\u001b[22m\u001b[39m\u001b[36m: catch this exception as `err` and call `code_typed(err; interactive = true)` to introspect the erronous code\u001b[39m",
     "output_type": "error",
     "traceback": [
      "InvalidIRError: compiling kernel #bad_kernel(CuDeviceVector{Int64, 1}) resulted in invalid LLVM IR\nReason: unsupported use of an undefined name (use of 'threadId')\nStacktrace:\n [1] \u001b[0m\u001b[1mbad_kernel\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[5]:2\u001b[24m\u001b[39m\nReason: unsupported dynamic function invocation\nStacktrace:\n [1] \u001b[0m\u001b[1mbad_kernel\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[5]:2\u001b[24m\u001b[39m\nReason: unsupported dynamic function invocation (call to getproperty)\nStacktrace:\n [1] \u001b[0m\u001b[1mbad_kernel\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[5]:2\u001b[24m\u001b[39m\nReason: unsupported dynamic function invocation (call to setindex!)\nStacktrace:\n [1] \u001b[0m\u001b[1mbad_kernel\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[5]:2\u001b[24m\u001b[39m\n\u001b[36m\u001b[1mHint\u001b[22m\u001b[39m\u001b[36m: catch this exception as `err` and call `code_typed(err; interactive = true)` to introspect the erronous code\u001b[39m",
      "",
      "Stacktrace:",
      "  [1] check_ir(job::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams, GPUCompiler.FunctionSpec{typeof(bad_kernel), Tuple{CuDeviceVector{Int64, 1}}}}, args::LLVM.Module)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/iaKrd/src/validation.jl:139",
      "  [2] macro expansion",
      "    @ ~/.julia/packages/GPUCompiler/iaKrd/src/driver.jl:414 [inlined]",
      "  [3] macro expansion",
      "    @ ~/.julia/packages/TimerOutputs/jgSVI/src/TimerOutput.jl:252 [inlined]",
      "  [4] macro expansion",
      "    @ ~/.julia/packages/GPUCompiler/iaKrd/src/driver.jl:412 [inlined]",
      "  [5] emit_asm(job::GPUCompiler.CompilerJob, ir::LLVM.Module; strip::Bool, validate::Bool, format::LLVM.API.LLVMCodeGenFileType)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/iaKrd/src/utils.jl:64",
      "  [6] cufunction_compile(job::GPUCompiler.CompilerJob, ctx::LLVM.Context)",
      "    @ CUDA ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:354",
      "  [7] #224",
      "    @ ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:347 [inlined]",
      "  [8] JuliaContext(f::CUDA.var\"#224#225\"{GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams, GPUCompiler.FunctionSpec{typeof(bad_kernel), Tuple{CuDeviceVector{Int64, 1}}}}})",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/iaKrd/src/driver.jl:74",
      "  [9] cufunction_compile(job::GPUCompiler.CompilerJob)",
      "    @ CUDA ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:346",
      " [10] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link))",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/iaKrd/src/cache.jl:90",
      " [11] cufunction(f::typeof(bad_kernel), tt::Type{Tuple{CuDeviceVector{Int64, 1}}}; name::Nothing, kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ CUDA ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:299",
      " [12] cufunction(f::typeof(bad_kernel), tt::Type{Tuple{CuDeviceVector{Int64, 1}}})",
      "    @ CUDA ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:293",
      " [13] top-level scope",
      "    @ ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:102",
      " [14] eval",
      "    @ ./boot.jl:373 [inlined]",
      " [15] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "function bad_kernel(a::AbstractArray)\n",
    "    a[threadId().x] = 0\n",
    "    return\n",
    "end\n",
    "\n",
    "@cuda bad_kernel(CuArray([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04681be",
   "metadata": {},
   "source": [
    "To inspect we can use `@code_warntype` or `code_llvm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c7ea3fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTX CompilerJob of kernel #bad_kernel(CuDeviceVector{Int64, 1}) for sm_80\n",
      "\n",
      "MethodInstance for bad_kernel(::CuDeviceVector{Int64, 1})\n",
      "  from bad_kernel(a::AbstractArray) in Main at In[5]:1\n",
      "Arguments\n",
      "  #self#\u001b[36m::Core.Const(bad_kernel)\u001b[39m\n",
      "  a\u001b[36m::CuDeviceVector{Int64, 1}\u001b[39m\n",
      "Body\u001b[36m::Nothing\u001b[39m\n",
      "\u001b[90m1 ─\u001b[39m %1 = Main.threadId()\u001b[91m\u001b[1m::Any\u001b[22m\u001b[39m\n",
      "\u001b[90m│  \u001b[39m %2 = Base.getproperty(%1, :x)\u001b[91m\u001b[1m::Any\u001b[22m\u001b[39m\n",
      "\u001b[90m│  \u001b[39m      Base.setindex!(a, 0, %2)\n",
      "\u001b[90m└──\u001b[39m      return nothing\n",
      "\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InvalidIRError: compiling kernel #bad_kernel(CuDeviceVector{Int64, 1}) resulted in invalid LLVM IR\nReason: unsupported use of an undefined name (use of 'threadId')\nStacktrace:\n [1] \u001b[0m\u001b[1mbad_kernel\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[5]:2\u001b[24m\u001b[39m\nReason: unsupported dynamic function invocation\nStacktrace:\n [1] \u001b[0m\u001b[1mbad_kernel\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[5]:2\u001b[24m\u001b[39m\nReason: unsupported dynamic function invocation (call to getproperty)\nStacktrace:\n [1] \u001b[0m\u001b[1mbad_kernel\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[5]:2\u001b[24m\u001b[39m\nReason: unsupported dynamic function invocation (call to setindex!)\nStacktrace:\n [1] \u001b[0m\u001b[1mbad_kernel\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[5]:2\u001b[24m\u001b[39m\n\u001b[36m\u001b[1mHint\u001b[22m\u001b[39m\u001b[36m: catch this exception as `err` and call `code_typed(err; interactive = true)` to introspect the erronous code\u001b[39m",
     "output_type": "error",
     "traceback": [
      "InvalidIRError: compiling kernel #bad_kernel(CuDeviceVector{Int64, 1}) resulted in invalid LLVM IR\nReason: unsupported use of an undefined name (use of 'threadId')\nStacktrace:\n [1] \u001b[0m\u001b[1mbad_kernel\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[5]:2\u001b[24m\u001b[39m\nReason: unsupported dynamic function invocation\nStacktrace:\n [1] \u001b[0m\u001b[1mbad_kernel\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[5]:2\u001b[24m\u001b[39m\nReason: unsupported dynamic function invocation (call to getproperty)\nStacktrace:\n [1] \u001b[0m\u001b[1mbad_kernel\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[5]:2\u001b[24m\u001b[39m\nReason: unsupported dynamic function invocation (call to setindex!)\nStacktrace:\n [1] \u001b[0m\u001b[1mbad_kernel\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[5]:2\u001b[24m\u001b[39m\n\u001b[36m\u001b[1mHint\u001b[22m\u001b[39m\u001b[36m: catch this exception as `err` and call `code_typed(err; interactive = true)` to introspect the erronous code\u001b[39m",
      "",
      "Stacktrace:",
      "  [1] check_ir(job::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams, GPUCompiler.FunctionSpec{typeof(bad_kernel), Tuple{CuDeviceVector{Int64, 1}}}}, args::LLVM.Module)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/iaKrd/src/validation.jl:139",
      "  [2] macro expansion",
      "    @ ~/.julia/packages/GPUCompiler/iaKrd/src/driver.jl:414 [inlined]",
      "  [3] macro expansion",
      "    @ ~/.julia/packages/TimerOutputs/jgSVI/src/TimerOutput.jl:252 [inlined]",
      "  [4] macro expansion",
      "    @ ~/.julia/packages/GPUCompiler/iaKrd/src/driver.jl:412 [inlined]",
      "  [5] emit_asm(job::GPUCompiler.CompilerJob, ir::LLVM.Module; strip::Bool, validate::Bool, format::LLVM.API.LLVMCodeGenFileType)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/iaKrd/src/utils.jl:64",
      "  [6] cufunction_compile(job::GPUCompiler.CompilerJob, ctx::LLVM.Context)",
      "    @ CUDA ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:354",
      "  [7] #224",
      "    @ ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:347 [inlined]",
      "  [8] JuliaContext(f::CUDA.var\"#224#225\"{GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams, GPUCompiler.FunctionSpec{typeof(bad_kernel), Tuple{CuDeviceVector{Int64, 1}}}}})",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/iaKrd/src/driver.jl:74",
      "  [9] cufunction_compile(job::GPUCompiler.CompilerJob)",
      "    @ CUDA ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:346",
      " [10] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link))",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/iaKrd/src/cache.jl:90",
      " [11] cufunction(f::typeof(bad_kernel), tt::Type{Tuple{CuDeviceVector{Int64, 1}}}; name::Nothing, kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ CUDA ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:299",
      " [12] cufunction(f::typeof(bad_kernel), tt::Type{Tuple{CuDeviceVector{Int64, 1}}})",
      "    @ CUDA ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:293",
      " [13] macro expansion",
      "    @ ~/.julia/packages/CUDA/tTK8Y/src/compiler/execution.jl:102 [inlined]",
      " [14] top-level scope",
      "    @ ~/.julia/packages/GPUCompiler/iaKrd/src/reflection.jl:201",
      " [15] eval",
      "    @ ./boot.jl:373 [inlined]",
      " [16] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "@device_code_warntype @cuda bad_kernel(CuArray([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ab0d0",
   "metadata": {},
   "source": [
    "From this view, it's much more clear that `threadId` was a typo and should be `threadIdx` instead. Note that in case your issue lies in a function that's called by the kernel, and isn't directly visible by the `@device_code_warntype` output, you can use Cthulhu.jl to interactively explore code by using `@device_code_warntype interactive=true` ... (the equivalent of Cthulhu's `@descend_code_warntype`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e35a0541",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "; PTX CompilerJob of kernel #good_kernel(CuDeviceVector{Int64, 1}) for sm_80\n",
      "\u001b[90m;  @ In[7]:1 within `good_kernel`\u001b[39m\n",
      "\u001b[95mdefine\u001b[39m \u001b[95mptx_kernel\u001b[39m \u001b[36mvoid\u001b[39m \u001b[93m@_Z22julia_good_kernel_657013CuDeviceArrayI5Int64Li1ELi1EE\u001b[39m\u001b[33m(\u001b[39m\u001b[33m[\u001b[39m\u001b[33m1\u001b[39m \u001b[0mx \u001b[36mi64\u001b[39m\u001b[33m]\u001b[39m \u001b[0m%state\u001b[0m, \u001b[33m{\u001b[39m \u001b[36mi8\u001b[39m \u001b[95maddrspace\u001b[39m\u001b[33m(\u001b[39m\u001b[33m1\u001b[39m\u001b[33m)\u001b[39m\u001b[0m*\u001b[0m, \u001b[36mi64\u001b[39m\u001b[0m, \u001b[33m[\u001b[39m\u001b[33m1\u001b[39m \u001b[0mx \u001b[36mi64\u001b[39m\u001b[33m]\u001b[39m\u001b[0m, \u001b[36mi64\u001b[39m \u001b[33m}\u001b[39m \u001b[0m%0\u001b[33m)\u001b[39m \u001b[95mlocal_unnamed_addr\u001b[39m \u001b[0m#1 \u001b[33m{\u001b[39m\n",
      "\u001b[91mconversion:\u001b[39m\n",
      "  \u001b[0m%.fca.0.extract \u001b[0m= \u001b[96m\u001b[1mextractvalue\u001b[22m\u001b[39m \u001b[33m{\u001b[39m \u001b[36mi8\u001b[39m \u001b[95maddrspace\u001b[39m\u001b[33m(\u001b[39m\u001b[33m1\u001b[39m\u001b[33m)\u001b[39m\u001b[0m*\u001b[0m, \u001b[36mi64\u001b[39m\u001b[0m, \u001b[33m[\u001b[39m\u001b[33m1\u001b[39m \u001b[0mx \u001b[36mi64\u001b[39m\u001b[33m]\u001b[39m\u001b[0m, \u001b[36mi64\u001b[39m \u001b[33m}\u001b[39m \u001b[0m%0\u001b[0m, \u001b[33m0\u001b[39m\n",
      "\u001b[90m;  @ In[7]:2 within `good_kernel`\u001b[39m\n",
      "\u001b[90m; ┌ @ /home/mvanzulli/.julia/packages/CUDA/tTK8Y/src/device/intrinsics/indexing.jl:92 within `#threadIdx`\u001b[39m\n",
      "\u001b[90m; │┌ @ /home/mvanzulli/.julia/packages/CUDA/tTK8Y/src/device/intrinsics/indexing.jl:46 within `threadIdx_x`\u001b[39m\n",
      "\u001b[90m; ││┌ @ /home/mvanzulli/.julia/packages/CUDA/tTK8Y/src/device/intrinsics/indexing.jl:6 within `_index`\u001b[39m\n",
      "\u001b[90m; │││┌ @ /home/mvanzulli/.julia/packages/CUDA/tTK8Y/src/device/intrinsics/indexing.jl:6 within `macro expansion` @ /home/mvanzulli/.julia/packages/LLVM/WjSQG/src/interop/base.jl:45\u001b[39m\n",
      "      \u001b[0m%1 \u001b[0m= \u001b[96m\u001b[1mcall\u001b[22m\u001b[39m \u001b[36mi32\u001b[39m \u001b[93m@llvm.nvvm.read.ptx.sreg.tid.x\u001b[39m\u001b[33m(\u001b[39m\u001b[33m)\u001b[39m\n",
      "\u001b[90m; └└└└\u001b[39m\n",
      "\u001b[90m; ┌ @ /home/mvanzulli/.julia/packages/CUDA/tTK8Y/src/device/array.jl:194 within `setindex!`\u001b[39m\n",
      "\u001b[90m; │┌ @ /home/mvanzulli/.julia/packages/CUDA/tTK8Y/src/device/array.jl:153 within `#arrayset`\u001b[39m\n",
      "\u001b[90m; ││┌ @ /home/mvanzulli/.julia/packages/CUDA/tTK8Y/src/device/array.jl:162 within `arrayset_bits`\u001b[39m\n",
      "\u001b[90m; │││┌ @ /home/mvanzulli/.julia/packages/LLVM/WjSQG/src/interop/pointer.jl:84 within `unsafe_store!`\u001b[39m\n",
      "\u001b[90m; ││││┌ @ /home/mvanzulli/.julia/packages/LLVM/WjSQG/src/interop/pointer.jl:44 within `pointerset`\u001b[39m\n",
      "\u001b[90m; │││││┌ @ /home/mvanzulli/.julia/packages/LLVM/WjSQG/src/interop/pointer.jl:44 within `macro expansion` @ /home/mvanzulli/.julia/packages/LLVM/WjSQG/src/interop/base.jl:45\u001b[39m\n",
      "\u001b[90m; ││││││┌ @ boot.jl:762 within `Int64`\u001b[39m\n",
      "\u001b[90m; │││││││┌ @ boot.jl:681 within `toInt64`\u001b[39m\n",
      "          \u001b[0m%2 \u001b[0m= \u001b[96m\u001b[1mzext\u001b[22m\u001b[39m \u001b[36mi32\u001b[39m \u001b[0m%1 \u001b[95mto\u001b[39m \u001b[36mi64\u001b[39m\n",
      "\u001b[90m; ││││││└└\u001b[39m\n",
      "        \u001b[0m%3 \u001b[0m= \u001b[96m\u001b[1mbitcast\u001b[22m\u001b[39m \u001b[36mi8\u001b[39m \u001b[95maddrspace\u001b[39m\u001b[33m(\u001b[39m\u001b[33m1\u001b[39m\u001b[33m)\u001b[39m\u001b[0m* \u001b[0m%.fca.0.extract \u001b[95mto\u001b[39m \u001b[36mi64\u001b[39m \u001b[95maddrspace\u001b[39m\u001b[33m(\u001b[39m\u001b[33m1\u001b[39m\u001b[33m)\u001b[39m\u001b[0m*\n",
      "        \u001b[0m%4 \u001b[0m= \u001b[96m\u001b[1mgetelementptr\u001b[22m\u001b[39m \u001b[95minbounds\u001b[39m \u001b[36mi64\u001b[39m\u001b[0m, \u001b[36mi64\u001b[39m \u001b[95maddrspace\u001b[39m\u001b[33m(\u001b[39m\u001b[33m1\u001b[39m\u001b[33m)\u001b[39m\u001b[0m* \u001b[0m%3\u001b[0m, \u001b[36mi64\u001b[39m \u001b[0m%2\n",
      "        \u001b[96m\u001b[1mstore\u001b[22m\u001b[39m \u001b[36mi64\u001b[39m \u001b[33m0\u001b[39m\u001b[0m, \u001b[36mi64\u001b[39m \u001b[95maddrspace\u001b[39m\u001b[33m(\u001b[39m\u001b[33m1\u001b[39m\u001b[33m)\u001b[39m\u001b[0m* \u001b[0m%4\u001b[0m, \u001b[95malign\u001b[39m \u001b[33m8\u001b[39m\n",
      "\u001b[90m; └└└└└└\u001b[39m\n",
      "\u001b[90m;  @ In[7]:3 within `good_kernel`\u001b[39m\n",
      "  \u001b[96m\u001b[1mret\u001b[22m\u001b[39m \u001b[36mvoid\u001b[39m\n",
      "\u001b[33m}\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "function good_kernel(a)\n",
    "    @inbounds a[threadIdx().x] = 0\n",
    "    return\n",
    "end\n",
    "\n",
    "@device_code_llvm @cuda good_kernel(CuArray([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cfd4f0",
   "metadata": {},
   "source": [
    "We can inspect more deeply even assembly code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4857c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m//\u001b[39;49;00m\n",
      "\u001b[90m// Generated by LLVM NVPTX Back-End\u001b[39;49;00m\n",
      "\u001b[90m//\u001b[39;49;00m\n",
      "\n",
      "\u001b[94m.version\u001b[39;49;00m \u001b[94m7.0\u001b[39;49;00m\n",
      "\u001b[94m.target\u001b[39;49;00m \u001b[91msm_80\u001b[39;49;00m\n",
      "\u001b[94m.address_size\u001b[39;49;00m \u001b[94m64\u001b[39;49;00m\n",
      "\n",
      "\t\u001b[90m// .globl\tjulia_good_kernel_6690  // -- Begin function julia_good_kernel_6690\u001b[39;49;00m\n",
      "                                        \u001b[90m// @julia_good_kernel_6690\u001b[39;49;00m\n",
      "\u001b[94m.visible\u001b[39;49;00m \u001b[94m.func\u001b[39;49;00m \u001b[91mjulia_good_kernel_6690\u001b[39;49;00m(\n",
      "\t\u001b[94m.param\u001b[39;49;00m \u001b[96m.b64\u001b[39;49;00m \u001b[91mjulia_good_kernel_6690_param_0\u001b[39;49;00m\n",
      ")\n",
      "{\n",
      "\t\u001b[94m.reg\u001b[39;49;00m \u001b[96m.b32\u001b[39;49;00m \t\u001b[91m%r\u001b[39;49;00m<\u001b[94m2\u001b[39;49;00m>;\n",
      "\t\u001b[94m.reg\u001b[39;49;00m \u001b[96m.b64\u001b[39;49;00m \t\u001b[91m%rd\u001b[39;49;00m<\u001b[94m6\u001b[39;49;00m>;\n",
      "\n",
      "\u001b[90m// %bb.0:                               // %top\u001b[39;49;00m\n",
      "\t\u001b[94mld.param.u64\u001b[39;49;00m \t\u001b[91m%rd1\u001b[39;49;00m, [\u001b[91mjulia_good_kernel_6690_param_0\u001b[39;49;00m];\n",
      "\t\u001b[94mmov.u32\u001b[39;49;00m \t\u001b[91m%r1\u001b[39;49;00m, \u001b[91m%tid\u001b[39;49;00m.\u001b[91mx\u001b[39;49;00m;\n",
      "\t\u001b[94mld.u64\u001b[39;49;00m \t\u001b[91m%rd2\u001b[39;49;00m, [\u001b[91m%rd1\u001b[39;49;00m];\n",
      "\t\u001b[94mmul.wide.u32\u001b[39;49;00m \t\u001b[91m%rd3\u001b[39;49;00m, \u001b[91m%r1\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m;\n",
      "\t\u001b[94madd.s64\u001b[39;49;00m \t\u001b[91m%rd4\u001b[39;49;00m, \u001b[91m%rd2\u001b[39;49;00m, \u001b[91m%rd3\u001b[39;49;00m;\n",
      "\t\u001b[94mmov.u64\u001b[39;49;00m \t\u001b[91m%rd5\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m;\n",
      "\t\u001b[94mst.global.u64\u001b[39;49;00m \t[\u001b[91m%rd4\u001b[39;49;00m], \u001b[91m%rd5\u001b[39;49;00m;\n",
      "\t\u001b[91mret\u001b[39;49;00m;\n",
      "                                        \u001b[90m// -- End function\u001b[39;49;00m\n",
      "}\n",
      "\t.headerflags\t@\"EF_CUDA_TEXMODE_UNIFIED EF_CUDA_64BIT_ADDRESS EF_CUDA_SM80 EF_CUDA_VIRTUAL_SM(EF_CUDA_SM80)\"\n",
      "\t.elftype\t@\"ET_EXEC\"\n",
      "\n",
      "\n",
      "//--------------------- .text._Z22julia_good_kernel_678613CuDeviceArrayI5Int64Li1ELi1EE --------------------------\n",
      "\t.section\t.text._Z22julia_good_kernel_678613CuDeviceArrayI5Int64Li1ELi1EE,\"ax\",@progbits\n",
      "\t.sectioninfo\t@\"SHI_REGISTERS=6\"\n",
      "\t.align\t128\n",
      "        .global         _Z22julia_good_kernel_678613CuDeviceArrayI5Int64Li1ELi1EE\n",
      "        .type           _Z22julia_good_kernel_678613CuDeviceArrayI5Int64Li1ELi1EE,@function\n",
      "        .size           _Z22julia_good_kernel_678613CuDeviceArrayI5Int64Li1ELi1EE,(.L_x_1 - _Z22julia_good_kernel_678613CuDeviceArrayI5Int64Li1ELi1EE)\n",
      "        .other          _Z22julia_good_kernel_678613CuDeviceArrayI5Int64Li1ELi1EE,@\"STO_CUDA_ENTRY STV_DEFAULT\"\n",
      "_Z22julia_good_kernel_678613CuDeviceArrayI5Int64Li1ELi1EE:\n",
      "\n",
      ".text._Z22julia_good_kernel_678613CuDeviceArrayI5Int64Li1ELi1EE:\n",
      "; Location ./In[7]:1\n",
      "        MOV R1, c[0x0][0x28] ;\n",
      "; Location /home/mvanzulli/.julia/packages/LLVM/WjSQG/src/interop/base.jl:45\n",
      "        S2R R2, SR_TID.X ;\n",
      "        HFMA2.MMA R3, -RZ, RZ, 0, 4.76837158203125e-07 ;\n",
      "        ULDC.64 UR4, c[0x0][0x118] ;\n",
      "        IMAD.WIDE.U32 R2, R2, R3, c[0x0][0x168] ;\n",
      "        STG.E.64 [R2.64], RZ ;\n",
      "; Location ./In[7]:3\n",
      "        EXIT ;\n",
      "\n",
      ".L_x_0:\n",
      "        BRA `(.L_x_0);\n",
      "        NOP;\n",
      "        NOP;\n",
      "        NOP;\n",
      "        NOP;\n",
      "        NOP;\n",
      "        NOP;\n",
      "        NOP;\n",
      "        NOP;\n",
      "\n",
      ".L_x_1:\n"
     ]
    }
   ],
   "source": [
    "CUDA.code_ptx(good_kernel, Tuple{CuDeviceVector{Int64, 1}})\n",
    "CUDA.code_sass(good_kernel, Tuple{CuDeviceVector{Int64, 1}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473bacf2",
   "metadata": {},
   "source": [
    "## Avoiding race condition problems "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bd43d1",
   "metadata": {},
   "source": [
    "It's also easy to run into issues related to parallel programming: threads need to be synchronized, memories initialized, etc. Mistakes are easy to make, for example, let's look at this kernel to transpose a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bcfdd2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32×32 CuArray{Int64, 2, CUDA.Mem.DeviceBuffer}:\n",
       "   1    2    3    4    5    6    7  …     0     0     0     0    31    32\n",
       "  33   34   35   36   37   38   39        0     0     0     0    63    64\n",
       "  65   66   67   68   69   70   71        0     0     0     0    95    96\n",
       "  97   98   99  100    0    0    0        0     0     0     0   127   128\n",
       " 129  130  131  132  133  134  135        0     0     0     0   159   160\n",
       " 161  162  163  164  165  166  167  …     0     0     0     0   191   192\n",
       " 193  194  195  196  197  198  199        0     0     0     0   223   224\n",
       " 225  226  227  228  229  230  231        0     0     0     0   255   256\n",
       " 257  258  259  260  261  262  263        0     0     0     0   287   288\n",
       " 289  290  291  292  293  294  295        0     0     0     0   319   320\n",
       " 321  322  323  324  325  326  327  …   347   348   349   350   351   352\n",
       " 353  354  355  356  357  358  359      379   380   381   382   383   384\n",
       " 385  386  387  388  389  390  391      411   412   413   414   415   416\n",
       "   ⋮                        ⋮       ⋱                             ⋮  \n",
       " 641  642  643  644  645  646  647  …   667   668   669   670   671   672\n",
       " 673  674  675  676  677  678  679      699   700   701   702   703   704\n",
       " 705  706  707  708  709  710  711      731   732   733   734   735   736\n",
       " 737  738  739  740  741  742  743      763   764   765   766   767   768\n",
       " 769  770  771  772  773  774  775      795   796   797   798   799   800\n",
       " 801  802  803  804  805  806  807  …   827   828   829   830   831   832\n",
       " 833  834  835  836  837  838  839      859   860   861   862   863   864\n",
       " 865  866  867  868  869  870  871      891   892   893   894   895   896\n",
       " 897  898  899  900  901  902  903      923   924   925   926   927   928\n",
       " 929  930  931  932  933  934  935      955   956   957   958   959   960\n",
       " 961  962  963  964  965  966  967  …   987   988   989   990   991   992\n",
       " 993  994  995  996  997  998  999     1019  1020  1021  1022  1023  1024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid transpose: false\n"
     ]
    }
   ],
   "source": [
    "const tile_dim = 16\n",
    "\n",
    "function gpu_transpose(input::CuMatrix)\n",
    "    function kernel(input::AbstractMatrix{T}, output::AbstractMatrix{T}) where {T}\n",
    "        # shared memory buffer so that operations to global memory are linear and can be coalesced\n",
    "        block = @cuStaticSharedMem(T, (tile_dim, tile_dim))\n",
    "\n",
    "        # read\n",
    "        x = tile_dim * (blockIdx().x - 1) + threadIdx().x\n",
    "        y = tile_dim * (blockIdx().y - 1) + threadIdx().y\n",
    "        if x <= size(input, 1) && y <= size(input, 2)\n",
    "            block[threadIdx().y, threadIdx().x] = input[x, y]\n",
    "        end\n",
    "        \n",
    "        # write\n",
    "        x = tile_dim * (blockIdx().y - 1) + threadIdx().x\n",
    "        y = tile_dim * (blockIdx().x - 1) + threadIdx().y\n",
    "        if x <= size(output, 1) && y <= size(output, 2)\n",
    "            output[x, y] = block[threadIdx().x, threadIdx().y]\n",
    "        end\n",
    "        return\n",
    "    end\n",
    "    \n",
    "    output = similar(input, reverse(size(input)))\n",
    "    @cuda threads=(tile_dim, tile_dim) blocks=(cld(size(input, 1), tile_dim), cld(size(input, 2), tile_dim)) kernel(input, output)\n",
    "    output\n",
    "end\n",
    "\n",
    "a = CuArray(reshape(1:1024, 32, 32))\n",
    "b = gpu_transpose(a)\n",
    "display(b)\n",
    "println(\"Valid transpose: \", Array(b) == Array(a)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc18ce7",
   "metadata": {},
   "source": [
    "If we execute this kernel a couple of times, we see some strange values in the matrix. This may be caused by a race condition, which NVIDIA has tools for to discover.\n",
    "\n",
    "With the compute sanitizer, https://docs.nvidia.com/cuda/compute-sanitizer/index.html, one can run julia under several sanitizer tools detecting issues like memory errors, race conditions, or missing synchronization. The sanitizer is shipped as part of CUDA.jl, but cannot be launched from within a session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbc4fa5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/home/mvanzulli/.julia/artifacts/913584335ab836f9781a0325178d0949c193f50b/bin/compute-sanitizer\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We obtain the kernel path \n",
    "CUDA.compute_sanitizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4aa312a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "AssertionError: Array(b) == Array(a)",
     "output_type": "error",
     "traceback": [
      "AssertionError: Array(b) == Array(a)",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[11]:5",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "# we create a new scritp\n",
    "using CUDA\n",
    "a = CUDA.ones(1024,1024)\n",
    "b = gpu_transpose(a)\n",
    "@assert Array(b) == Array(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d3944",
   "metadata": {},
   "source": [
    "and we can execute:\n",
    "```bash\n",
    "/home/mvanzulli/.julia/artifacts/913584335ab836f9781a0325178d0949c193f50b/bin/compute-sanitizer --launch-timeout=0 --report-api-errors=no --tool=racecheck julia /home/mvanzulli/Repositories/gitHub/Julia_by_Notebooks/cuda/transpose_race_conflict.jl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a6c35",
   "metadata": {},
   "source": [
    "the result will be "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc608e",
   "metadata": {},
   "source": [
    "```bash\n",
    "========= COMPUTE-SANITIZER\n",
    "32×32 CuArray{Int64, 2, CUDA.Mem.DeviceBuffer}:\n",
    "========= Error: Race reported between Write access at 0x410 in /home/mvanzulli/.julia/packages/LLVM/WjSQG/src/interop/base.jl:45:julia_kernel_1680(CuDeviceArray<Int64, (int)2, (int)1>, CuDeviceArray<Int64, (int)2, (int)1>)\n",
    "=========     and Read access at 0x750 in int.jl:87:julia_kernel_1680(CuDeviceArray<Int64, (int)2, (int)1>, CuDeviceArray<Int64, (int)2, (int)1>) [7680 hazards]\n",
    "=========\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddad812d",
   "metadata": {},
   "source": [
    "So we have a race condition problem. Note the `--launch-timeout=0` argument, to give Julia some time to perform the first CUDA API call, and `--report-api-errors=no` to hide probably unrelated API errors (that can come from a variety of places, including NVIDIA's own libraries). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7513b6b",
   "metadata": {},
   "source": [
    "To fix this we just add a `syncthreads()` between read and write operations and that's it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97356de",
   "metadata": {},
   "source": [
    "### Define an array inside a kernel for each thread \n",
    "\n",
    "Since this is not supported many fallbacks arise: \n",
    "- Use StaticArrays\n",
    "- Use Shared memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2cd51c95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×3 MMatrix{3, 3, Int32, 9} with indices SOneTo(3)×SOneTo(3):\n",
       " 1           0  2\n",
       " 0  -566635184  0\n",
       " 1       32556  1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b, c, d) is (1, 1, 1)\n",
      "(b, c, d) is (2, 1, 1)\n",
      "(b, c, d) is (1, 2, 1)\n",
      "(b, c, d) is (2, 2, 1)\n",
      "(b, c, d) is (1, 1, 2)\n",
      "(b, c, d) is (2, 1, 2)\n",
      "(b, c, d) is (1, 2, 2)\n",
      "(b, c, d) is (2, 2, 2)\n",
      "(b, c, d) is (1, 1, 1)\n",
      "(b, c, d) is (2, 1, 1)\n",
      "(b, c, d) is (1, 2, 1)\n",
      "(b, c, d) is (2, 2, 1)\n",
      "(b, c, d) is (1, 1, 2)\n",
      "(b, c, d) is (2, 1, 2)\n",
      "(b, c, d) is (1, 2, 2)\n",
      "(b, c, d) is (2, 2, 2)\n",
      "(b, c, d) is (1, 1, 1)\n",
      "(b, c, d) is (2, 1, 1)\n",
      "(b, c, d) is (1, 2, 1)\n",
      "(b, c, d) is (2, 2, 1)\n",
      "(b, c, d) is (1, 1, 2)\n",
      "(b, c, d) is (2, 1, 2)\n",
      "(b, c, d) is (1, 2, 2)\n",
      "(b, c, d) is (2, 2, 2)\n",
      "(b, c, d) is (1, 1, 1)\n",
      "(b, c, d) is (2, 1, 1)\n",
      "(b, c, d) is (1, 2, 1)\n",
      "(b, c, d) is (2, 2, 1)\n",
      "(b, c, d) is (1, 1, 2)\n",
      "(b, c, d) is (2, 1, 2)\n",
      "(b, c, d) is (1, 2, 2)\n",
      "(b, c, d) is (2, 2, 2)\n",
      "(b, c, d) is (1, 1, 1)\n",
      "(b, c, d) is (2, 1, 1)\n",
      "(b, c, d) is (1, 2, 1)\n",
      "(b, c, d) is (2, 2, 1)\n",
      "(b, c, d) is (1, 1, 2)\n",
      "(b, c, d) is (2, 1, 2)\n",
      "(b, c, d) is (1, 2, 2)\n",
      "(b, c, d) is (2, 2, 2)\n",
      "(b, c, d) is (1, 1, 1)\n",
      "(b, c, d) is (2, 1, 1)\n",
      "(b, c, d) is (1, 2, 1)\n",
      "(b, c, d) is (2, 2, 1)\n",
      "(b, c, d) is (1, 1, 2)\n",
      "(b, c, d) is (2, 1, 2)\n",
      "(b, c, d) is (1, 2, 2)\n",
      "(b, c, d) is (2, 2, 2)\n",
      "(b, c, d) is (1, 1, 1)\n",
      "(b, c, d) is (2, 1, 1)\n",
      "(b, c, d) is (1, 2, 1)\n",
      "(b, c, d) is (2, 2, 1)\n",
      "(b, c, d) is (1, 1, 2)\n",
      "(b, c, d) is (2, 1, 2)\n",
      "(b, c, d) is (1, 2, 2)\n",
      "(b, c, d) is (2, 2, 2)\n",
      "(b, c, d) is (1, 1, 1)\n",
      "(b, c, d) is (2, 1, 1)\n",
      "(b, c, d) is (1, 2, 1)\n",
      "(b, c, d) is (2, 2, 1)\n",
      "(b, c, d) is (1, 1, 2)\n",
      "(b, c, d) is (2, 1, 2)\n",
      "(b, c, d) is (1, 2, 2)\n",
      "(b, c, d) is (2, 2, 2)\n",
      "(f) is ((2, 2, 2))\n",
      "(f) is ((3, 2, 2))\n",
      "(f) is ((2, 3, 2))\n",
      "(f) is ((3, 3, 2))\n",
      "(f) is ((2, 2, 3))\n",
      "(f) is ((3, 2, 3))\n",
      "(f) is ((2, 3, 3))\n",
      "(f) is ((3, 3, 3))\n",
      "(f) is ((2, 2, 2))\n",
      "(f) is ((3, 2, 2))\n",
      "(f) is ((2, 3, 2))\n",
      "(f) is ((3, 3, 2))\n",
      "(f) is ((2, 2, 3))\n",
      "(f) is ((3, 2, 3))\n",
      "(f) is ((2, 3, 3))\n",
      "(f) is ((3, 3, 3))\n",
      "(f) is ((2, 2, 2))\n",
      "(f) is ((3, 2, 2))\n",
      "(f) is ((2, 3, 2))\n",
      "(f) is ((3, 3, 2))\n",
      "(f) is ((2, 2, 3))\n",
      "(f) is ((3, 2, 3))\n",
      "(f) is ((2, 3, 3))\n",
      "(f) is ((3, 3, 3))\n",
      "(f) is ((2, 2, 2))\n",
      "(f) is ((3, 2, 2))\n",
      "(f) is ((2, 3, 2))\n",
      "(f) is ((3, 3, 2))\n",
      "(f) is ((2, 2, 3))\n",
      "(f) is ((3, 2, 3))\n",
      "(f) is ((2, 3, 3))\n",
      "(f) is ((3, 3, 3))\n",
      "(f) is ((2, 2, 2))\n",
      "(f) is ((3, 2, 2))\n",
      "(f) is ((2, 3, 2))\n",
      "(f) is ((3, 3, 2))\n",
      "(f) is ((2, 2, 3))\n",
      "(f) is ((3, 2, 3))\n",
      "(f) is ((2, 3, 3))\n",
      "(f) is ((3, 3, 3))\n",
      "(f) is ((2, 2, 2))\n",
      "(f) is ((3, 2, 2))\n",
      "(f) is ((2, 3, 2))\n",
      "(f) is ((3, 3, 2))\n",
      "(f) is ((2, 2, 3))\n",
      "(f) is ((3, 2, 3))\n",
      "(f) is ((2, 3, 3))\n",
      "(f) is ((3, 3, 3))\n",
      "(f) is ((2, 2, 2))\n",
      "(f) is ((3, 2, 2))\n",
      "(f) is ((2, 3, 2))\n",
      "(f) is ((3, 3, 2))\n",
      "(f) is ((2, 2, 3))\n",
      "(f) is ((3, 2, 3))\n",
      "(f) is ((2, 3, 3))\n",
      "(f) is ((3, 3, 3))\n",
      "(f) is ((2, 2, 2))\n",
      "(f) is ((3, 2, 2))\n",
      "(f) is ((2, 3, 2))\n",
      "(f) is ((3, 3, 2))\n",
      "(f) is ((2, 2, 3))\n",
      "(f) is ((3, 2, 3))\n",
      "(f) is ((2, 3, 3))\n",
      "(f) is ((3, 3, 3))\n"
     ]
    }
   ],
   "source": [
    "using CUDA, StaticArrays\n",
    "const mat_indexes_rows = 1\n",
    "const mat_indexes_cols = 3\n",
    "function bug_kernel(a::AbstractArray)\n",
    "    \n",
    "    b = threadIdx().x\n",
    "    c = threadIdx().y\n",
    "    d = threadIdx().z\n",
    "    @cuprintln(\"(b, c, d) is ($b, $c, $d)\")\n",
    "        # define a vector\n",
    "    f = [b+1, c+1, d+1]\n",
    "    f = b+1, c+1, d+1\n",
    "    @cuprintln(\"(f) is ($f)\")\n",
    "\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "# Define box\n",
    "N = 3\n",
    "box = (N, N, N)\n",
    "# Define grid\n",
    "dim_block = (2, 2, 2)\n",
    "# Define num threads\n",
    "num_threads = cld.(box, dim_block)\n",
    "\n",
    "random_array_for_thread = MMatrix{3, 3, Int32}(undef)\n",
    "\n",
    "@device_code_typed CUDA.@cuda(\n",
    "    threads = num_threads,\n",
    "    blocks = dim_block,\n",
    "    bug_kernel(CuArray(random_array_for_thread))\n",
    ")\n",
    "\n",
    "random_array_for_thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a43e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
