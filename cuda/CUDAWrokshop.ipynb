{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6028f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=Philosopher:400,700,400italic,700italic' rel='stylesheet' type='text/css'>\r\n",
       "\r\n",
       "<style>\r\n",
       "\r\n",
       "@font-face {\r\n",
       "    font-family: \"Computer Modern\";\r\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "\r\n",
       "/* Formatting for header cells */\r\n",
       ".text_cell_render h1 {\r\n",
       "    font-family: 'Philosopher', sans-serif;\r\n",
       "    font-weight: 400;\r\n",
       "    font-size: 2.2em;\r\n",
       "    line-height: 100%;\r\n",
       "    color: rgb(0, 80, 120);\r\n",
       "    margin-bottom: 0.1em;\r\n",
       "    margin-top: 0.1em;\r\n",
       "    display: block;\r\n",
       "}\t\r\n",
       ".text_cell_render h2 {\r\n",
       "    font-family: 'Philosopher', serif;\r\n",
       "    font-weight: 400;\r\n",
       "    font-size: 1.9em;\r\n",
       "    line-height: 100%;\r\n",
       "    color: rgb(200,100,0);\r\n",
       "    margin-bottom: 0.1em;\r\n",
       "    margin-top: 0.1em;\r\n",
       "    display: block;\r\n",
       "}\t\r\n",
       "\r\n",
       ".text_cell_render h3 {\r\n",
       "    font-family: 'Philosopher', serif;\r\n",
       "    margin-top:12px;\r\n",
       "    margin-bottom: 3px;\r\n",
       "    font-style: italic;\r\n",
       "    color: rgb(94,127,192);\r\n",
       "}\r\n",
       "\r\n",
       ".text_cell_render h4 {\r\n",
       "    font-family: 'Philosopher', serif;\r\n",
       "}\r\n",
       "\r\n",
       ".text_cell_render h5 {\r\n",
       "    font-family: 'Alegreya Sans', sans-serif;\r\n",
       "    font-weight: 300;\r\n",
       "    font-size: 16pt;\r\n",
       "    color: grey;\r\n",
       "    font-style: italic;\r\n",
       "    margin-bottom: .1em;\r\n",
       "    margin-top: 0.1em;\r\n",
       "    display: block;\r\n",
       "}\r\n",
       "\r\n",
       ".text_cell_render h6 {\r\n",
       "    font-family: 'PT Mono', sans-serif;\r\n",
       "    font-weight: 300;\r\n",
       "    font-size: 10pt;\r\n",
       "    color: grey;\r\n",
       "    margin-bottom: 1px;\r\n",
       "    margin-top: 1px;\r\n",
       "}\r\n",
       "\r\n",
       ".CodeMirror{\r\n",
       "        font-family: \"PT Mono\";\r\n",
       "        font-size: 100%;\r\n",
       "}\r\n",
       "\r\n",
       "</style>\r\n",
       "\r\n"
      ],
      "text/plain": [
       "HTML{String}(\"<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\\r\\n<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\\r\\n<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\\r\\n<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\\r\\n<link href='http://fonts.googleapis.com/css?family=Philosopher:400,700,400italic,700italic' rel='stylesheet' type='text/css'>\\r\\n\\r\\n<style>\\r\\n\\r\\n@font-face {\\r\\n    font-family: \\\"Computer Modern\\\";\\r\\n    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\\r\\n}\\r\\n\\r\\n\\r\\n\\r\\n/* Formatting for header cells */\\r\\n.text_cell_render h1 {\\r\\n    font-family: 'Philosopher', sans-serif;\\r\\n    font-weight: 400;\\r\\n    font-size: 2.2em;\\r\\n    line-height: 100%;\\r\\n    color: rgb(0, 80, 120);\\r\\n    margin-bottom: 0.1em;\\r\\n    margin-top: 0.1em;\\r\\n    display: block;\\r\\n}\\t\\r\\n.text_cell_render h2 {\\r\\n    font-family: 'Philosopher', serif;\\r\\n    font-weight: 400;\\r\\n    font-size: 1.9em;\\r\\n    line-height: 100%;\\r\\n    color: rgb(200,100,0);\\r\\n    margin-bottom: 0.1em;\\r\\n    margin-top: 0.1em;\\r\\n    display: block;\\r\\n}\\t\\r\\n\\r\\n.text_cell_render h3 {\\r\\n    font-family: 'Philosopher', serif;\\r\\n    margin-top:12px;\\r\\n    margin-bottom: 3px;\\r\\n    font-style: italic;\\r\\n    color: rgb(94,127,192);\\r\\n}\\r\\n\\r\\n.text_cell_render h4 {\\r\\n    font-family: 'Philosopher', serif;\\r\\n}\\r\\n\\r\\n.text_cell_render h5 {\\r\\n    font-family: 'Alegreya Sans', sans-serif;\\r\\n    font-weight: 300;\\r\\n    font-size: 16pt;\\r\\n    color: grey;\\r\\n    font-style: italic;\\r\\n    margin-bottom: .1em;\\r\\n    margin-top: 0.1em;\\r\\n    display: block;\\r\\n}\\r\\n\\r\\n.text_cell_render h6 {\\r\\n    font-family: 'PT Mono', sans-serif;\\r\\n    font-weight: 300;\\r\\n    font-size: 10pt;\\r\\n    color: grey;\\r\\n    margin-bottom: 1px;\\r\\n    margin-top: 1px;\\r\\n}\\r\\n\\r\\n.CodeMirror{\\r\\n        font-family: \\\"PT Mono\\\";\\r\\n        font-size: 100%;\\r\\n}\\r\\n\\r\\n</style>\\r\\n\\r\\n\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Setting up a custom stylesheet in IJulia\n",
    "file = open(\"./../style.css\") # A .css file in the same folder as this notebook file\n",
    "styl = read(file, String) # Read the file\n",
    "HTML(\"$styl\") # Output as HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e590de1c",
   "metadata": {},
   "source": [
    "## juliaGPU (based on the [juliaGPU 2021 workshop](https://www.youtube.com/watch?v=Hz9IMJuW5hU))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e398e",
   "metadata": {},
   "source": [
    "We have different packages available at juliaGPU organization [gitHub](https://cuda.juliagpu.org/stable/) but only some of them are useful:\n",
    "\n",
    "* Infrastructure: `GPUArrays.jl` `Adapt.jl` `GPUComplier.jl`\n",
    "* Library wrappers: `OpenCL.jl` `VulkanCore.jl` `MetalCore.jl`\n",
    "* Programming frameworks (useful): `CUDA.jl`, `AMDGPU.jl`,`oneAPI.jl` and `ArrayFire.jl`(C library)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb872bce",
   "metadata": {},
   "source": [
    "### Back end \n",
    "Analogsly to Julia with `LLVM.jl` a real time complier that generates machine code into the CPU they use `GPUComplier.jl`, then `LLVM.jl` create `LLVM.jl` to generate GPU code.  \n",
    "### Architecture \n",
    "From Host to Device the data is transferred via PCI/Express bus which is actually really slow, thus we want to avoid copying data from CPU to GPU. Then the CPU inside has a lot of CPU SM streaming multi-processors that can access to shared mem (similar to cahce L2), registers (cache L1) or global memory. Occupancy is the Nvidia term of how to manage all the memory available by a program. \n",
    "### Implicit parallelism\n",
    "There are inner functions that are already implemented using parallelism like `CUDA.reduce()`, and we don't have to worry about implementing it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad75554",
   "metadata": {},
   "source": [
    "<img src=arch.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc55a5ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA toolkit 11.7, artifact installation\n",
      "NVIDIA driver 515.57.0, for CUDA 11.7\n",
      "CUDA driver 11.7\n",
      "\n",
      "Libraries: \n",
      "- CUBLAS: 11.10.1\n",
      "- CURAND: 10.2.10\n",
      "- CUFFT: 10.7.2\n",
      "- CUSOLVER: 11.3.5\n",
      "- CUSPARSE: 11.7.3\n",
      "- CUPTI: 17.0.0\n",
      "- NVML: 11.0.0+515.57\n",
      "- CUDNN: 8.30.2 (for CUDA 11.5.0)\n",
      "- CUTENSOR: 1.4.0 (for CUDA 11.5.0)\n",
      "\n",
      "Toolchain:\n",
      "- Julia: 1.7.3\n",
      "- LLVM: 12.0.1\n",
      "- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0\n",
      "- Device capability support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80\n",
      "\n",
      "1 device:\n",
      "  0: NVIDIA GeForce RTX 3060 Laptop GPU (sm_86, 5.598 GiB / 6.000 GiB available)\n"
     ]
    }
   ],
   "source": [
    "# load CUDA.jl\n",
    "import Pkg\n",
    "using CUDA\n",
    "\n",
    "# check out the CUDA version\n",
    "CUDA.versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc4a34a",
   "metadata": {},
   "source": [
    "### Options for CUDA selection\n",
    "There's two special environment variables that can currently be used to influence the CUDA toolkit that's used:\n",
    "\n",
    "* `JULIA_CUDA_VERSION` that sets the CUDA version that we want to install (this is useful if any update version is not working with our set up) \n",
    "* `JULIA_CUDA_USE_BINARYBUILDER` set to false will install \n",
    "\n",
    "Besides we have this customization variables is not recommended to change because CUDA.jl will automatically select the correct one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa6a6ff",
   "metadata": {},
   "source": [
    "### High order abstraction \n",
    "As CUBLAS or other libraries are C libraries are limited in terms of which types it supports so we need a generic fallbacks. Although the CuArray type and its support for NVIDIA's vendor libraries is nice, it isn't really novel and could as well have been implemented in Python (and libraries like CuPy do exactly that). The real flexibility of Julia's arrays comes from its higher-order abstractions, where you combine the abstraction with custom code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80458fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = CUDA.ones(5)\n",
    "# aply a function to each element of a\n",
    "y = broadcast(a) do x\n",
    "    2x\n",
    "end\n",
    "# or what is the same \n",
    "y = a.*2\n",
    "# we have map \n",
    "y = map(a) do x\n",
    "    2x\n",
    "end\n",
    "# also accumulate and reduce \n",
    "red = reduce(+, a)\n",
    "acc = accumulate(*,a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a520c40",
   "metadata": {},
   "source": [
    "Here it is important to highlight that CUDA.jl doesn't provides a tensor complier, as consequence we can't merge functions like accumulate(reduce) for instance. If we merge operations then will be launching as many kernels as nesting operations are executed. For instance what happen if we want to sum each column of a CuArray and divide into the number of elements of each column matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b183117",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m147.644 μs\u001b[22m\u001b[39m … \u001b[35m 37.570 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 37.59%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m155.161 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m182.644 μs\u001b[22m\u001b[39m ± \u001b[32m764.426 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m3.27% ±  0.78%\n",
       "\n",
       "  \u001b[39m▇\u001b[34m█\u001b[39m\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[32m▂\u001b[39m\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\n",
       "  \u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▄\u001b[39m▃\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▄\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▁\u001b[39m▃\u001b[39m▄\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▃\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m▃\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m \u001b[39m█\n",
       "  148 μs\u001b[90m        \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m        512 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m23.39 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m501\u001b[39m."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = CUDA.ones(10,10)\n",
    "\n",
    "y = broadcast(eachcol(a)) do x\n",
    "    sum(x)/length(eachcol(a))\n",
    "end\n",
    "# this will create 10 kernels one per each column of a \n",
    "# let's mesure the performance \n",
    "using BenchmarkTools \n",
    "@benchmark CUDA.@sync y = broadcast(eachcol(a)) do x\n",
    "                        sum(x)/length(eachcol(a))\n",
    "                      end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6416b55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m12.387 μs\u001b[22m\u001b[39m … \u001b[35m484.670 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m14.688 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m17.060 μs\u001b[22m\u001b[39m ± \u001b[32m 13.778 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m▇\u001b[39m█\u001b[39m▄\u001b[39m▂\u001b[34m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▂\u001b[39m▅\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m▇\u001b[39m▅\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[32m▃\u001b[39m\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m▂\n",
       "  12.4 μs\u001b[90m         Histogram: frequency by time\u001b[39m         35.5 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m2.70 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m57\u001b[39m."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another possiblity is to use dims keyword like\n",
    "sum(a; dims=2) / size(a,2)\n",
    "\n",
    "using BenchmarkTools \n",
    "@benchmark CUDA.@sync sum(a; dims=2) / size(a,2)\n",
    "# is arround 10 times faster since only one kernel is complied and executed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3918d4aa",
   "metadata": {},
   "source": [
    "### More complicated applications can be faced using Tulio.jl, a tensor compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4705b4f",
   "metadata": {},
   "source": [
    "### Reduce example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2da97a",
   "metadata": {},
   "source": [
    "We might start with a serial simple for implementation for one single thread;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b856c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: \u001b[90m#= In[5]:18 =#\u001b[39m CUDA.@allowscalar d_b[] == sum(c_a)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function reduce_singlethread(op, a, b)\n",
    "    for i in 1:length(a)\n",
    "        b[] = op(b[], a[i])\n",
    "    end\n",
    "    return\n",
    "end\n",
    "# define variables\n",
    "c_a = 1:16\n",
    "d_a = CuArray(1:16)\n",
    "d_b = CuArray([0])\n",
    "# lunch kernel\n",
    "@cuda(\n",
    "    threads = 1,\n",
    "    reduce_singlethread(+, d_a, d_b)\n",
    "    )\n",
    "# test the result \n",
    "using Test\n",
    "@test CUDA.@allowscalar d_b[] == sum(c_a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87244b84",
   "metadata": {},
   "source": [
    "Obviously this not a very efficient implementation since is a serial one (only one thread). We can test it using `@sync` macro to force the CPU to wait until GPU completes its work. Otherwise we will be measuring the time to queue an operation and not the time it takes to finish it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "648341d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 17 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m311.749 ms\u001b[22m\u001b[39m … \u001b[35m312.385 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m312.064 ms               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m312.069 ms\u001b[22m\u001b[39m ± \u001b[32m184.298 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m█\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m█\u001b[34m \u001b[39m\u001b[39m▁\u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m▁\u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \n",
       "  \u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m█\u001b[34m▁\u001b[39m\u001b[39m█\u001b[32m▁\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m \u001b[39m▁\n",
       "  312 ms\u001b[90m           Histogram: frequency by time\u001b[39m          312 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m3.42 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m62\u001b[39m."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 2048\n",
    "@benchmark CUDA.@sync @cuda( \n",
    "    threads = 1, \n",
    "        reduce_singlethread(+,   $(CUDA.rand(N, N)), $(CUDA.rand(N, N)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded943f3",
   "metadata": {},
   "source": [
    "Other option is to use `CUDA.@atomic` to take advantage of the SMPD paradigm: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3785b68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: \u001b[90m#= In[7]:18 =#\u001b[39m CUDA.@allowscalar d_b[] == sum(c_a)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function reduce_atomic(op, a, b)\n",
    "    index = threadIdx().x + (blockIdx().x -1)*blockDim().x\n",
    "    @inbounds CUDA.@atomic b[] = b[] + a[index]\n",
    "    return\n",
    "end\n",
    "# define variables\n",
    "c_a = 1:16\n",
    "d_a = CuArray(1:16)\n",
    "d_b = CuArray([0])\n",
    "# lunch kernel\n",
    "@cuda(\n",
    "    threads = 16,\n",
    "    blocks = 1,\n",
    "    reduce_singlethread(+, d_a, d_b)\n",
    "    )\n",
    "# test the result \n",
    "using Test\n",
    "@test CUDA.@allowscalar d_b[] == sum(c_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dccffd3",
   "metadata": {},
   "source": [
    "Let's now benchmark our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7ea4e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 3 evaluations.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m7.839 μs\u001b[22m\u001b[39m … \u001b[35m142.948 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m8.600 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m9.244 μs\u001b[22m\u001b[39m ± \u001b[32m  4.011 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m▁\u001b[39m▆\u001b[39m▇\u001b[39m█\u001b[34m▇\u001b[39m\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[32m▄\u001b[39m\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\n",
       "  \u001b[39m▃\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▅\u001b[39m▆\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▅\u001b[39m▆\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▄\u001b[39m▆\u001b[39m▄\u001b[39m▅\u001b[39m▆\u001b[39m▇\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▅\u001b[39m▃\u001b[39m▅\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m \u001b[39m█\n",
       "  7.84 μs\u001b[90m      \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m      16.6 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m304 bytes\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m5\u001b[39m."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 2048\n",
    "@benchmark CUDA.@sync @cuda( \n",
    "    threads = 1024, \n",
    "    blocks = 2,\n",
    "        reduce_atomic(+,   $(CUDA.rand(N, N)), $(CUDA.rand(N, N)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603fb9e8",
   "metadata": {},
   "source": [
    "but we can still improving it using a parallel reduce strategy like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbdd61d",
   "metadata": {},
   "source": [
    "<img src=parallel_reduction.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680193c",
   "metadata": {},
   "source": [
    "The main idea is that each thread reduces its flowing element and so on, finally a the value is written in global mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0616cb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread 1: a[1] + a[2] = 3\n",
      "thread 2: a[3] + a[4] = 7\n",
      "thread 3: a[5] + a[6] = 11\n",
      "thread 4: a[7] + a[8] = 15\n",
      "thread 5: a[9] + a[10] = 19\n",
      "thread 6: a[11] + a[12] = 23\n",
      "thread 7: a[13] + a[14] = 27\n",
      "thread 8: a[15] + a[16] = 31\n",
      "stride = 2\n",
      "thread 1: a[1] + a[3] = 10\n",
      "thread 2: a[5] + a[7] = 26\n",
      "thread 3: a[9] + a[11] = 42\n",
      "thread 4: a[13] + a[15] = 58\n",
      "stride = 4\n",
      "thread 1: a[1] + a[5] = 36\n",
      "thread 2: a[9] + a[13] = 100\n",
      "stride = 8\n",
      "thread 1: a[1] + a[9] = 136\n",
      "stride = 16\n",
      "stride = 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: \u001b[90m#= In[9]:41 =#\u001b[39m CUDA.@allowscalar d_b[] == sum(c_a)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets start with a single block reduction \n",
    "\n",
    "function reduce_block(op, a, b)\n",
    "    num_elements = blockDim().x*2\n",
    "    thread = threadIdx().x\n",
    "    \n",
    "    #parallel reduction of values in a block (stride or distance between each thread reduction) \n",
    "    stride = 1\n",
    "    # while still have elements to reduce \n",
    "    while stride < num_elements\n",
    "        # add a barrier to sync threads\n",
    "        sync_threads()\n",
    "        # compute index to reduce \n",
    "        index = 2*stride*(thread - 1) + 1 \n",
    "        # check index and index + d are inbounds a\n",
    "        @inbounds if index ≤ num_elements && index + stride ≤ length(a)\n",
    "            CUDA.@cuprintln (\"thread $thread: a[$index] + a[$(index + stride)] = $(a[index] + a[index + stride])\")\n",
    "            a[index] = op(a[index], a[index + stride])\n",
    "        end\n",
    "        stride *= 2\n",
    "        thread == 1 && CUDA.@cuprintln(\"stride = $stride\")\n",
    "    end\n",
    "    # download the block reduction to global mem\n",
    "    if thread == 1 \n",
    "        b[] = a[1]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "c_a = 1:16\n",
    "d_a = CuArray(1:16)\n",
    "d_b = CuArray([0])\n",
    "# lunch kernel\n",
    "@cuda(\n",
    "    threads = 16,\n",
    "    blocks = 1,\n",
    "    reduce_block(+, d_a, d_b)\n",
    "    )\n",
    "# test the result \n",
    "using Test\n",
    "@test CUDA.@allowscalar d_b[] == sum(c_a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552911d6",
   "metadata": {},
   "source": [
    "We want that different threads access to consecutive memory items (coalesced access pattern). Now we can extend the implementation for a general grid with multiple blocks. Now, we do the reduction for each block and then do an atomic operation to a global mem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ec8c4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: \u001b[90m#= In[10]:45 =#\u001b[39m CUDA.@allowscalar d_b[] == sum(c_a)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function reduce_grid_atomic(op, a, b)\n",
    "    num_elements = blockDim().x*2\n",
    "    thread = threadIdx().x\n",
    "    block = blockIdx().x\n",
    "    \n",
    "    #parallel reduction of values in a block (stride or distance between each thread reduction) \n",
    "    stride_threads = 1\n",
    "    # parallel reduction between blocks has a stride of \n",
    "    stride_blocks = (block - 1)*num_elements\n",
    "\n",
    "    \n",
    "    # while still have elements to reduce \n",
    "    while stride_threads < num_elements\n",
    "        # add a barrier to sync threads\n",
    "        sync_threads()\n",
    "        # compute index to reduce \n",
    "        index = 2*stride_threads*(thread - 1) + 1 \n",
    "        # check index and index + d are inbounds a\n",
    "        @inbounds if index ≤ num_elements && index + stride_threads + stride_blocks ≤ length(a)\n",
    "#             CUDA.@cuprintln (\"thread $thread: a[$index] + a[$(index + stride_blocks)] = $(a[index] + a[index + stride_blocks])\")\n",
    "            a[stride_blocks + index] = op(a[index + stride_blocks], a[index + stride_threads + stride_blocks])\n",
    "        end\n",
    "        stride_threads *= 2\n",
    "    end\n",
    "    # do attomic operatios with the first entry of ech block (sum through each block)\n",
    "    if thread == 1 \n",
    "        CUDA.@atomic b[] = op(b[], a[stride_blocks + 1])\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "# define test inputs\n",
    "c_a = 1:16\n",
    "d_a = CuArray(1:16)\n",
    "d_b = CuArray([0])\n",
    "# lunch kernel\n",
    "@cuda(\n",
    "    threads = 2,\n",
    "    blocks = 4,\n",
    "    reduce_grid_atomic(+, d_a, d_b)\n",
    "    )\n",
    "# test the result \n",
    "using Test\n",
    "CUDA.@allowscalar d_b\n",
    "@test CUDA.@allowscalar d_b[] == sum(c_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0870ab3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 4 evaluations.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m7.207 μs\u001b[22m\u001b[39m … \u001b[35m127.096 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m7.829 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m8.714 μs\u001b[22m\u001b[39m ± \u001b[32m  3.798 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m▁\u001b[39m█\u001b[39m▇\u001b[39m▆\u001b[34m▅\u001b[39m\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[32m▅\u001b[39m\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\n",
       "  \u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▄\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▆\u001b[39m█\u001b[39m█\u001b[39m▅\u001b[39m▆\u001b[39m▆\u001b[39m▄\u001b[39m▆\u001b[39m▆\u001b[39m \u001b[39m█\n",
       "  7.21 μs\u001b[90m      \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m      17.1 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m304 bytes\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m5\u001b[39m."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 2048\n",
    "@benchmark CUDA.@sync @cuda( \n",
    "    threads = 1024, \n",
    "    blocks = 2,\n",
    "        reduce_grid_atomic(+,   $(CUDA.rand(N, N)), $(CUDA.rand(N, N)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc71c400",
   "metadata": {},
   "source": [
    "Another option is to implement the reduce technique using shared memory that each multiprocessor has, the reduce is done into a shared memory variable and finally an atomic operation is used to download the data to global memory. The shared memory is used as a cache memory and also to communicate between threads. We can use shared memory as a buffer (we should avoid here bank conflicts (at the same operation threads access to the same bank slot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef6f0b4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: \u001b[90m#= In[12]:46 =#\u001b[39m CUDA.@allowscalar d_b[] == sum(c_a)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function reduce_grid_shared(op, a::AbstractArray{T}, b) where {T}\n",
    "    num_elements = blockDim().x*2\n",
    "    thread = threadIdx().x\n",
    "    block = blockIdx().x\n",
    "    #parallel reduction of values in a block (stride or distance between each thread reduction) \n",
    "    stride_threads = 1\n",
    "    # parallel reduction between blocks has a stride of \n",
    "    stride_blocks = (block - 1)*num_elements\n",
    "    \n",
    "    # shared mem to buffer the a elements\n",
    "    shared = @cuStaticSharedMem(T, (2048,))\n",
    "    @inbounds shared[thread] = a[thread + stride_blocks]\n",
    "    @inbounds shared[thread + blockDim().x] = a[thread + stride_blocks + blockDim().x]\n",
    " \n",
    "    # while still have elements to reduce \n",
    "    while stride_threads < num_elements\n",
    "        # add a barrier to sync threads\n",
    "        sync_threads()\n",
    "        # compute index to reduce \n",
    "        index = 2*stride_threads*(thread - 1) + 1 \n",
    "        # check index and index + d are inbounds a\n",
    "        @inbounds if index ≤ num_elements && index + stride_threads + stride_blocks ≤ length(a)\n",
    "            shared[index] = op(shared[index], shared[index + stride_threads])\n",
    "        end\n",
    "        stride_threads *= 2\n",
    "    end\n",
    "    # do attomic operatios with the first entry of ech block reduction at shared \n",
    "    if thread == 1 \n",
    "        CUDA.@atomic b[] = op(b[], shared[1])\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "# define test inputs\n",
    "c_a = 1:16\n",
    "d_a = CuArray(1:16)\n",
    "d_b = CuArray([0])\n",
    "# lunch kernel\n",
    "@cuda(\n",
    "    threads = 4,\n",
    "    blocks = 2,\n",
    "    reduce_grid_shared(+, d_a, d_b)\n",
    "    )\n",
    "# test the result \n",
    "using Test\n",
    "CUDA.@allowscalar d_b\n",
    "@test CUDA.@allowscalar d_b[] == sum(c_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "452781ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 4 evaluations.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m7.584 μs\u001b[22m\u001b[39m … \u001b[35m120.274 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m8.307 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m9.144 μs\u001b[22m\u001b[39m ± \u001b[32m  3.643 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m0.00% ± 0.00%\n",
       "\n",
       "  \u001b[39m \u001b[39m▅\u001b[39m▆\u001b[39m█\u001b[34m▇\u001b[39m\u001b[39m▆\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[32m▄\u001b[39m\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\n",
       "  \u001b[39m▄\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▄\u001b[39m▅\u001b[39m▄\u001b[39m▃\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▆\u001b[39m█\u001b[39m█\u001b[39m▃\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▆\u001b[39m▇\u001b[39m█\u001b[39m \u001b[39m█\n",
       "  7.58 μs\u001b[90m      \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m      17.4 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m304 bytes\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m5\u001b[39m."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test it \n",
    "N = 2048\n",
    "@benchmark CUDA.@sync @cuda( \n",
    "    threads = 1024, \n",
    "    blocks = 2,\n",
    "        reduce_grid_shared(+,   $(CUDA.rand(N, N)), $(CUDA.rand(N, N)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95f6b76",
   "metadata": {},
   "source": [
    "### With new GPU's shared memory is not a big advantage in essence, since atomic operations to global memory are already optimezed. But we can use the sum operation and test it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a20b8582",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m 86.281 μs\u001b[22m\u001b[39m … \u001b[35m 44.019 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 36.11%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m 97.918 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m107.170 μs\u001b[22m\u001b[39m ± \u001b[32m440.431 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m1.48% ±  0.36%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▆\u001b[39m█\u001b[39m▇\u001b[34m▄\u001b[39m\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▂\u001b[39m▂\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▃\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▄\u001b[32m▃\u001b[39m\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▃\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m \u001b[39m▃\n",
       "  86.3 μs\u001b[90m          Histogram: frequency by time\u001b[39m          140 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m4.67 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m90\u001b[39m."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 2048\n",
    "@benchmark CUDA.@sync CUDA.sum($(CUDA.rand(N, N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e0bb41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 356 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m 8.171 ms\u001b[22m\u001b[39m … \u001b[35m22.544 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m 0.00% … 37.76%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m13.681 ms              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m13.95%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m14.065 ms\u001b[22m\u001b[39m ± \u001b[32m 3.451 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m18.43% ± 15.61%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m▅\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▅\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\u001b[39m▂\u001b[39m█\u001b[39m \u001b[39m \u001b[39m█\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▃\u001b[39m \u001b[34m \u001b[39m\u001b[39m \u001b[32m▂\u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▆\u001b[39m▄\u001b[39m▁\u001b[39m▅\u001b[39m▇\u001b[39m▅\u001b[39m \u001b[39m \u001b[39m \u001b[39m▃\u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▅\u001b[39m▆\u001b[39m█\u001b[39m█\u001b[39m▅\u001b[39m▇\u001b[39m▆\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▄\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▅\u001b[34m█\u001b[39m\u001b[39m▇\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m▆\u001b[39m█\u001b[39m▃\u001b[39m▄\u001b[39m▅\u001b[39m▇\u001b[39m▅\u001b[39m█\u001b[39m▅\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m▆\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▆\u001b[39m▆\u001b[39m \u001b[39m▅\n",
       "  8.17 ms\u001b[90m         Histogram: frequency by time\u001b[39m        20.9 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m32.00 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m3\u001b[39m."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark sum(rand(N,N))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
